{
  "run_info": {
    "run_name": "tft_dict_factored_wiki_6L6H768D",
    "start_time": "2025-06-04T20:06:35.814710",
    "status": "completed",
    "end_time": "2025-06-05T01:18:51.622588",
    "final_metrics": {
      "total_training_time": 18735.805684804916,
      "total_steps": 29166,
      "accelerator_state": {
        "num_processes": 4,
        "process_index": 0,
        "device": "cuda:0",
        "mixed_precision": "bf16",
        "gradient_accumulation_steps": 1,
        "is_main_process": true
      },
      "num_epochs": 3,
      "current_epoch": 3,
      "current_batch_idx": 9721,
      "loss": 2.677904749156957,
      "epoch_duration": 6228.5077114105225,
      "status": "Completed",
      "epoch_losses": [
        3.2493942396014446,
        2.7652897583410785,
        2.677904749156957
      ],
      "final_loss": 2.677904749156957,
      "training_time": 18735.805557966232,
      "final_perplexity": 14.554565859343455
    }
  },
  "config": {
    "training": {
      "accelerator_state": {
        "num_processes": 4,
        "process_index": 0,
        "device": "cuda:0",
        "mixed_precision": "bf16",
        "gradient_accumulation_steps": 1,
        "is_main_process": true
      },
      "num_epochs": 3,
      "current_epoch": 3,
      "current_batch_idx": 9721,
      "loss": 2.677904749156957,
      "epoch_duration": 6228.5077114105225,
      "status": "Completed",
      "epoch_losses": [
        3.2493942396014446,
        2.7652897583410785,
        2.677904749156957
      ],
      "final_loss": 2.677904749156957,
      "training_time": 18735.805557966232
    },
    "system": {
      "device": "cuda:0",
      "num_epochs": null
    }
  },
  "metrics": {
    "train": [],
    "eval": [],
    "epochs": [
      {
        "epoch": 1,
        "timestamp": 1749088270.923654,
        "duration": 6275.106712579727,
        "loss": 3.2493942396014446,
        "perplexity": 25.774721881477294,
        "accelerator_state": {
          "num_processes": 4,
          "process_index": 0,
          "device": "cuda:0",
          "mixed_precision": "bf16",
          "gradient_accumulation_steps": 1,
          "is_main_process": true
        },
        "num_epochs": 3,
        "current_epoch": 1,
        "current_batch_idx": 9721,
        "epoch_duration": 6275.105140209198
      },
      {
        "epoch": 2,
        "timestamp": 1749094500.0874968,
        "duration": 6227.140292167664,
        "loss": 2.7652897583410785,
        "perplexity": 15.883641741565077,
        "accelerator_state": {
          "num_processes": 4,
          "process_index": 0,
          "device": "cuda:0",
          "mixed_precision": "bf16",
          "gradient_accumulation_steps": 1,
          "is_main_process": true
        },
        "num_epochs": 3,
        "current_epoch": 2,
        "current_batch_idx": 9721,
        "epoch_duration": 6227.136559724808
      },
      {
        "epoch": 3,
        "timestamp": 1749100730.0868225,
        "duration": 6228.514485120773,
        "loss": 2.677904749156957,
        "perplexity": 14.554565859343455,
        "accelerator_state": {
          "num_processes": 4,
          "process_index": 0,
          "device": "cuda:0",
          "mixed_precision": "bf16",
          "gradient_accumulation_steps": 1,
          "is_main_process": true
        },
        "num_epochs": 3,
        "current_epoch": 3,
        "current_batch_idx": 9721,
        "epoch_duration": 6228.5077114105225
      }
    ],
    "steps": [
      {
        "step": 50,
        "timestamp": 1749082029.3492658,
        "phase": "train",
        "loss": 7.186541557312012,
        "learning_rate": 0.0005,
        "perplexity": 1321.5248751823385
      },
      {
        "step": 100,
        "timestamp": 1749082062.1087074,
        "phase": "train",
        "loss": 6.416825294494629,
        "learning_rate": 0.0005,
        "perplexity": 612.0569259805275
      },
      {
        "step": 150,
        "timestamp": 1749082094.4978786,
        "phase": "train",
        "loss": 6.18999719619751,
        "learning_rate": 0.0005,
        "perplexity": 487.84473839223045
      },
      {
        "step": 200,
        "timestamp": 1749082127.1538484,
        "phase": "train",
        "loss": 5.764163017272949,
        "learning_rate": 0.0005,
        "perplexity": 318.6722092490382
      },
      {
        "step": 250,
        "timestamp": 1749082159.719772,
        "phase": "train",
        "loss": 5.474118232727051,
        "learning_rate": 0.0005,
        "perplexity": 238.44012550896554
      },
      {
        "step": 300,
        "timestamp": 1749082192.0588763,
        "phase": "train",
        "loss": 5.213973522186279,
        "learning_rate": 0.0005,
        "perplexity": 183.82303383743152
      },
      {
        "step": 350,
        "timestamp": 1749082224.1209352,
        "phase": "train",
        "loss": 5.0901288986206055,
        "learning_rate": 0.0005,
        "perplexity": 162.4107952318081
      },
      {
        "step": 400,
        "timestamp": 1749082256.6709213,
        "phase": "train",
        "loss": 4.891438007354736,
        "learning_rate": 0.0005,
        "perplexity": 133.14489979955394
      },
      {
        "step": 450,
        "timestamp": 1749082288.9650826,
        "phase": "train",
        "loss": 4.748602867126465,
        "learning_rate": 0.0005,
        "perplexity": 115.42291068021383
      },
      {
        "step": 500,
        "timestamp": 1749082321.9074903,
        "phase": "train",
        "loss": 4.600035190582275,
        "learning_rate": 0.0005,
        "perplexity": 99.48781661452878
      },
      {
        "step": 550,
        "timestamp": 1749082354.9508488,
        "phase": "train",
        "loss": 4.402289867401123,
        "learning_rate": 0.0005,
        "perplexity": 81.63759406052421
      },
      {
        "step": 600,
        "timestamp": 1749082387.870429,
        "phase": "train",
        "loss": 4.371154308319092,
        "learning_rate": 0.0005,
        "perplexity": 79.1349251014363
      },
      {
        "step": 650,
        "timestamp": 1749082420.1017091,
        "phase": "train",
        "loss": 4.240838527679443,
        "learning_rate": 0.0005,
        "perplexity": 69.46607665183976
      },
      {
        "step": 700,
        "timestamp": 1749082452.1500816,
        "phase": "train",
        "loss": 4.160038471221924,
        "learning_rate": 0.0005,
        "perplexity": 64.07398755711623
      },
      {
        "step": 750,
        "timestamp": 1749082484.2987573,
        "phase": "train",
        "loss": 4.056951522827148,
        "learning_rate": 0.0005,
        "perplexity": 57.7978468253078
      },
      {
        "step": 800,
        "timestamp": 1749082516.5574627,
        "phase": "train",
        "loss": 4.009299278259277,
        "learning_rate": 0.0005,
        "perplexity": 55.10824148802428
      },
      {
        "step": 850,
        "timestamp": 1749082549.3839674,
        "phase": "train",
        "loss": 3.9696831703186035,
        "learning_rate": 0.0005,
        "perplexity": 52.9677464266918
      },
      {
        "step": 900,
        "timestamp": 1749082581.534578,
        "phase": "train",
        "loss": 3.885071277618408,
        "learning_rate": 0.0005,
        "perplexity": 48.67041144687325
      },
      {
        "step": 950,
        "timestamp": 1749082614.4810138,
        "phase": "train",
        "loss": 3.8993639945983887,
        "learning_rate": 0.0005,
        "perplexity": 49.37103887064455
      },
      {
        "step": 1000,
        "timestamp": 1749082646.5447922,
        "phase": "train",
        "loss": 3.764758586883545,
        "learning_rate": 0.0005,
        "perplexity": 43.15328683280614
      },
      {
        "step": 1050,
        "timestamp": 1749082679.2431068,
        "phase": "train",
        "loss": 3.7604265213012695,
        "learning_rate": 0.0005,
        "perplexity": 42.966748304452295
      },
      {
        "step": 1100,
        "timestamp": 1749082711.8331633,
        "phase": "train",
        "loss": 3.752686023712158,
        "learning_rate": 0.0005,
        "perplexity": 42.635448160931546
      },
      {
        "step": 1150,
        "timestamp": 1749082744.47969,
        "phase": "train",
        "loss": 3.7555768489837646,
        "learning_rate": 0.0005,
        "perplexity": 42.75887811319674
      },
      {
        "step": 1200,
        "timestamp": 1749082777.075757,
        "phase": "train",
        "loss": 3.648031711578369,
        "learning_rate": 0.0005,
        "perplexity": 38.39901128894075
      },
      {
        "step": 1250,
        "timestamp": 1749082809.2064018,
        "phase": "train",
        "loss": 3.6037392616271973,
        "learning_rate": 0.0005,
        "perplexity": 36.73534099623898
      },
      {
        "step": 1300,
        "timestamp": 1749082841.9191573,
        "phase": "train",
        "loss": 3.5491080284118652,
        "learning_rate": 0.0005,
        "perplexity": 34.782278842367106
      },
      {
        "step": 1350,
        "timestamp": 1749082874.153273,
        "phase": "train",
        "loss": 3.5670716762542725,
        "learning_rate": 0.0005,
        "perplexity": 35.412741198816505
      },
      {
        "step": 1400,
        "timestamp": 1749082906.4373057,
        "phase": "train",
        "loss": 3.5456953048706055,
        "learning_rate": 0.0005,
        "perplexity": 34.6637788593948
      },
      {
        "step": 1450,
        "timestamp": 1749082938.9158375,
        "phase": "train",
        "loss": 3.5771820545196533,
        "learning_rate": 0.0005,
        "perplexity": 35.77259346368051
      },
      {
        "step": 1500,
        "timestamp": 1749082971.4378855,
        "phase": "train",
        "loss": 3.421264171600342,
        "learning_rate": 0.0005,
        "perplexity": 30.608084444602515
      },
      {
        "step": 1550,
        "timestamp": 1749083003.8643124,
        "phase": "train",
        "loss": 3.4076883792877197,
        "learning_rate": 0.0005,
        "perplexity": 30.195363294169077
      },
      {
        "step": 1600,
        "timestamp": 1749083036.2643552,
        "phase": "train",
        "loss": 3.508801221847534,
        "learning_rate": 0.0005,
        "perplexity": 33.408194755422215
      },
      {
        "step": 1650,
        "timestamp": 1749083068.8382635,
        "phase": "train",
        "loss": 3.417137622833252,
        "learning_rate": 0.0005,
        "perplexity": 30.482038936804834
      },
      {
        "step": 1700,
        "timestamp": 1749083101.4175618,
        "phase": "train",
        "loss": 3.477914810180664,
        "learning_rate": 0.0005,
        "perplexity": 32.3921079125342
      },
      {
        "step": 1750,
        "timestamp": 1749083133.7613356,
        "phase": "train",
        "loss": 3.429835319519043,
        "learning_rate": 0.0005,
        "perplexity": 30.871558387957744
      },
      {
        "step": 1800,
        "timestamp": 1749083166.2512176,
        "phase": "train",
        "loss": 3.4424691200256348,
        "learning_rate": 0.0005,
        "perplexity": 31.26405765603593
      },
      {
        "step": 1850,
        "timestamp": 1749083198.8270037,
        "phase": "train",
        "loss": 3.295499563217163,
        "learning_rate": 0.0005,
        "perplexity": 26.99089436051164
      },
      {
        "step": 1900,
        "timestamp": 1749083231.192937,
        "phase": "train",
        "loss": 3.4367599487304688,
        "learning_rate": 0.0005,
        "perplexity": 31.086074347535146
      },
      {
        "step": 1950,
        "timestamp": 1749083263.3236353,
        "phase": "train",
        "loss": 3.334197998046875,
        "learning_rate": 0.0005,
        "perplexity": 28.055873333309584
      },
      {
        "step": 2000,
        "timestamp": 1749083296.6983192,
        "phase": "train",
        "loss": 3.300045967102051,
        "learning_rate": 0.0005,
        "perplexity": 27.113885238742615
      },
      {
        "step": 2050,
        "timestamp": 1749083329.035246,
        "phase": "train",
        "loss": 3.3478341102600098,
        "learning_rate": 0.0005,
        "perplexity": 28.44106667144494
      },
      {
        "step": 2100,
        "timestamp": 1749083361.5242178,
        "phase": "train",
        "loss": 3.3235599994659424,
        "learning_rate": 0.0005,
        "perplexity": 27.758996877971526
      },
      {
        "step": 2150,
        "timestamp": 1749083393.7234433,
        "phase": "train",
        "loss": 3.2920548915863037,
        "learning_rate": 0.0005,
        "perplexity": 26.898079542478296
      },
      {
        "step": 2200,
        "timestamp": 1749083426.3907537,
        "phase": "train",
        "loss": 3.3102548122406006,
        "learning_rate": 0.0005,
        "perplexity": 27.39210442620757
      },
      {
        "step": 2250,
        "timestamp": 1749083458.7049706,
        "phase": "train",
        "loss": 3.274411916732788,
        "learning_rate": 0.0005,
        "perplexity": 26.427679233818395
      },
      {
        "step": 2300,
        "timestamp": 1749083490.925472,
        "phase": "train",
        "loss": 3.290733814239502,
        "learning_rate": 0.0005,
        "perplexity": 26.86256856046382
      },
      {
        "step": 2350,
        "timestamp": 1749083522.9625776,
        "phase": "train",
        "loss": 3.337270975112915,
        "learning_rate": 0.0005,
        "perplexity": 28.142220992869582
      },
      {
        "step": 2400,
        "timestamp": 1749083555.2464135,
        "phase": "train",
        "loss": 3.252497434616089,
        "learning_rate": 0.0005,
        "perplexity": 25.85483010135238
      },
      {
        "step": 2450,
        "timestamp": 1749083588.163182,
        "phase": "train",
        "loss": 3.3153271675109863,
        "learning_rate": 0.0005,
        "perplexity": 27.5313998908444
      },
      {
        "step": 2500,
        "timestamp": 1749083620.4137995,
        "phase": "train",
        "loss": 3.200498342514038,
        "learning_rate": 0.0005,
        "perplexity": 24.544758846658393
      },
      {
        "step": 2550,
        "timestamp": 1749083653.423436,
        "phase": "train",
        "loss": 3.2067995071411133,
        "learning_rate": 0.0005,
        "perplexity": 24.699907708799813
      },
      {
        "step": 2600,
        "timestamp": 1749083685.3971279,
        "phase": "train",
        "loss": 3.116534948348999,
        "learning_rate": 0.0005,
        "perplexity": 22.56804456392949
      },
      {
        "step": 2650,
        "timestamp": 1749083718.6456492,
        "phase": "train",
        "loss": 3.250206708908081,
        "learning_rate": 0.0005,
        "perplexity": 25.79567156122656
      },
      {
        "step": 2700,
        "timestamp": 1749083751.015493,
        "phase": "train",
        "loss": 3.1426596641540527,
        "learning_rate": 0.0005,
        "perplexity": 23.16539717394206
      },
      {
        "step": 2750,
        "timestamp": 1749083783.3258917,
        "phase": "train",
        "loss": 3.1535401344299316,
        "learning_rate": 0.0005,
        "perplexity": 23.41882378955473
      },
      {
        "step": 2800,
        "timestamp": 1749083816.1417205,
        "phase": "train",
        "loss": 3.280824899673462,
        "learning_rate": 0.0005,
        "perplexity": 26.59770409045093
      },
      {
        "step": 2850,
        "timestamp": 1749083848.623721,
        "phase": "train",
        "loss": 3.2019925117492676,
        "learning_rate": 0.0005,
        "perplexity": 24.58146028246321
      },
      {
        "step": 2900,
        "timestamp": 1749083880.808269,
        "phase": "train",
        "loss": 3.15134334564209,
        "learning_rate": 0.0005,
        "perplexity": 23.36743404690136
      },
      {
        "step": 2950,
        "timestamp": 1749083913.5227032,
        "phase": "train",
        "loss": 3.1800880432128906,
        "learning_rate": 0.0005,
        "perplexity": 24.048870798710038
      },
      {
        "step": 3000,
        "timestamp": 1749083946.0036838,
        "phase": "train",
        "loss": 3.1051552295684814,
        "learning_rate": 0.0005,
        "perplexity": 22.31268229450834
      },
      {
        "step": 3050,
        "timestamp": 1749083978.7919605,
        "phase": "train",
        "loss": 3.1513025760650635,
        "learning_rate": 0.0005,
        "perplexity": 23.366481385919002
      },
      {
        "step": 3100,
        "timestamp": 1749084011.4016654,
        "phase": "train",
        "loss": 3.195193290710449,
        "learning_rate": 0.0005,
        "perplexity": 24.414892408137565
      },
      {
        "step": 3150,
        "timestamp": 1749084043.6117733,
        "phase": "train",
        "loss": 3.1650750637054443,
        "learning_rate": 0.0005,
        "perplexity": 23.69052226447501
      },
      {
        "step": 3200,
        "timestamp": 1749084075.7148194,
        "phase": "train",
        "loss": 3.205746650695801,
        "learning_rate": 0.0005,
        "perplexity": 24.67391593697323
      },
      {
        "step": 3250,
        "timestamp": 1749084108.1982672,
        "phase": "train",
        "loss": 3.036862850189209,
        "learning_rate": 0.0005,
        "perplexity": 20.83976311940253
      },
      {
        "step": 3300,
        "timestamp": 1749084140.4804761,
        "phase": "train",
        "loss": 3.0453121662139893,
        "learning_rate": 0.0005,
        "perplexity": 21.016590848572523
      },
      {
        "step": 3350,
        "timestamp": 1749084172.8827999,
        "phase": "train",
        "loss": 3.053846597671509,
        "learning_rate": 0.0005,
        "perplexity": 21.19672307234992
      },
      {
        "step": 3400,
        "timestamp": 1749084205.3747702,
        "phase": "train",
        "loss": 3.052713632583618,
        "learning_rate": 0.0005,
        "perplexity": 21.172721524156724
      },
      {
        "step": 3450,
        "timestamp": 1749084238.2763047,
        "phase": "train",
        "loss": 3.1268579959869385,
        "learning_rate": 0.0005,
        "perplexity": 22.802222196891115
      },
      {
        "step": 3500,
        "timestamp": 1749084271.0203128,
        "phase": "train",
        "loss": 3.1951308250427246,
        "learning_rate": 0.0005,
        "perplexity": 24.41336736321284
      },
      {
        "step": 3550,
        "timestamp": 1749084303.817068,
        "phase": "train",
        "loss": 3.0419809818267822,
        "learning_rate": 0.0005,
        "perplexity": 20.946697188233408
      },
      {
        "step": 3600,
        "timestamp": 1749084336.0969548,
        "phase": "train",
        "loss": 3.1351873874664307,
        "learning_rate": 0.0005,
        "perplexity": 22.992944027899355
      },
      {
        "step": 3650,
        "timestamp": 1749084368.5969622,
        "phase": "train",
        "loss": 3.1083428859710693,
        "learning_rate": 0.0005,
        "perplexity": 22.383920940924796
      },
      {
        "step": 3700,
        "timestamp": 1749084400.8840125,
        "phase": "train",
        "loss": 3.2073211669921875,
        "learning_rate": 0.0005,
        "perplexity": 24.7127960203418
      },
      {
        "step": 3750,
        "timestamp": 1749084433.2557864,
        "phase": "train",
        "loss": 3.061011791229248,
        "learning_rate": 0.0005,
        "perplexity": 21.349147117726794
      },
      {
        "step": 3800,
        "timestamp": 1749084465.6805773,
        "phase": "train",
        "loss": 3.1213467121124268,
        "learning_rate": 0.0005,
        "perplexity": 22.676898342281866
      },
      {
        "step": 3850,
        "timestamp": 1749084498.135908,
        "phase": "train",
        "loss": 3.151085138320923,
        "learning_rate": 0.0005,
        "perplexity": 23.36140118325186
      },
      {
        "step": 3900,
        "timestamp": 1749084530.7008774,
        "phase": "train",
        "loss": 3.05391001701355,
        "learning_rate": 0.0005,
        "perplexity": 21.19806739720824
      },
      {
        "step": 3950,
        "timestamp": 1749084563.4544532,
        "phase": "train",
        "loss": 3.0925698280334473,
        "learning_rate": 0.0005,
        "perplexity": 22.033627917530225
      },
      {
        "step": 4000,
        "timestamp": 1749084596.2653196,
        "phase": "train",
        "loss": 3.1685843467712402,
        "learning_rate": 0.0005,
        "perplexity": 23.773805059058528
      },
      {
        "step": 4050,
        "timestamp": 1749084628.6856012,
        "phase": "train",
        "loss": 3.0337347984313965,
        "learning_rate": 0.0005,
        "perplexity": 20.77467711101658
      },
      {
        "step": 4100,
        "timestamp": 1749084661.261658,
        "phase": "train",
        "loss": 2.9962921142578125,
        "learning_rate": 0.0005,
        "perplexity": 20.011199948877536
      },
      {
        "step": 4150,
        "timestamp": 1749084693.9716432,
        "phase": "train",
        "loss": 3.0567541122436523,
        "learning_rate": 0.0005,
        "perplexity": 21.258442535203773
      },
      {
        "step": 4200,
        "timestamp": 1749084726.162478,
        "phase": "train",
        "loss": 3.124852418899536,
        "learning_rate": 0.0005,
        "perplexity": 22.75653641100751
      },
      {
        "step": 4250,
        "timestamp": 1749084758.3369894,
        "phase": "train",
        "loss": 3.052274703979492,
        "learning_rate": 0.0005,
        "perplexity": 21.163430250304682
      },
      {
        "step": 4300,
        "timestamp": 1749084790.326345,
        "phase": "train",
        "loss": 2.9628758430480957,
        "learning_rate": 0.0005,
        "perplexity": 19.353549571651705
      },
      {
        "step": 4350,
        "timestamp": 1749084822.3262024,
        "phase": "train",
        "loss": 3.057744026184082,
        "learning_rate": 0.0005,
        "perplexity": 21.279496983147638
      },
      {
        "step": 4400,
        "timestamp": 1749084855.0428426,
        "phase": "train",
        "loss": 3.1248555183410645,
        "learning_rate": 0.0005,
        "perplexity": 22.75660694367081
      },
      {
        "step": 4450,
        "timestamp": 1749084887.5957673,
        "phase": "train",
        "loss": 2.926955223083496,
        "learning_rate": 0.0005,
        "perplexity": 18.670695760406435
      },
      {
        "step": 4500,
        "timestamp": 1749084920.0509326,
        "phase": "train",
        "loss": 2.9626264572143555,
        "learning_rate": 0.0005,
        "perplexity": 19.34872367233642
      },
      {
        "step": 4550,
        "timestamp": 1749084952.4633274,
        "phase": "train",
        "loss": 2.958984851837158,
        "learning_rate": 0.0005,
        "perplexity": 19.278391395093543
      },
      {
        "step": 4600,
        "timestamp": 1749084984.6453588,
        "phase": "train",
        "loss": 2.8827009201049805,
        "learning_rate": 0.0005,
        "perplexity": 17.862453144026624
      },
      {
        "step": 4650,
        "timestamp": 1749085016.7772186,
        "phase": "train",
        "loss": 2.995439052581787,
        "learning_rate": 0.0005,
        "perplexity": 19.99413644025728
      },
      {
        "step": 4700,
        "timestamp": 1749085048.9137735,
        "phase": "train",
        "loss": 3.0989227294921875,
        "learning_rate": 0.0005,
        "perplexity": 22.174050959059876
      },
      {
        "step": 4750,
        "timestamp": 1749085080.9298866,
        "phase": "train",
        "loss": 2.975721836090088,
        "learning_rate": 0.0005,
        "perplexity": 19.603768851437525
      },
      {
        "step": 4800,
        "timestamp": 1749085113.0311582,
        "phase": "train",
        "loss": 3.076955795288086,
        "learning_rate": 0.0005,
        "perplexity": 21.692266082412644
      },
      {
        "step": 4850,
        "timestamp": 1749085144.7316222,
        "phase": "train",
        "loss": 3.044250965118408,
        "learning_rate": 0.0005,
        "perplexity": 20.99429984904711
      },
      {
        "step": 4900,
        "timestamp": 1749085177.1794012,
        "phase": "train",
        "loss": 3.1252634525299072,
        "learning_rate": 0.0005,
        "perplexity": 22.76589203538961
      },
      {
        "step": 4950,
        "timestamp": 1749085209.613021,
        "phase": "train",
        "loss": 2.951564311981201,
        "learning_rate": 0.0005,
        "perplexity": 19.13586478957727
      },
      {
        "step": 5000,
        "timestamp": 1749085242.2998724,
        "phase": "train",
        "loss": 3.082627773284912,
        "learning_rate": 0.0005,
        "perplexity": 21.81565373355931
      },
      {
        "step": 5050,
        "timestamp": 1749085274.519986,
        "phase": "train",
        "loss": 2.9914329051971436,
        "learning_rate": 0.0005,
        "perplexity": 19.914197213923657
      },
      {
        "step": 5100,
        "timestamp": 1749085307.2316768,
        "phase": "train",
        "loss": 2.9895331859588623,
        "learning_rate": 0.0005,
        "perplexity": 19.87640174212067
      },
      {
        "step": 5150,
        "timestamp": 1749085339.4391272,
        "phase": "train",
        "loss": 3.0280256271362305,
        "learning_rate": 0.0005,
        "perplexity": 20.656408848913998
      },
      {
        "step": 5200,
        "timestamp": 1749085371.391858,
        "phase": "train",
        "loss": 2.913079261779785,
        "learning_rate": 0.0005,
        "perplexity": 18.413411072874712
      },
      {
        "step": 5250,
        "timestamp": 1749085403.9182363,
        "phase": "train",
        "loss": 2.9829025268554688,
        "learning_rate": 0.0005,
        "perplexity": 19.74504407319651
      },
      {
        "step": 5300,
        "timestamp": 1749085436.0293348,
        "phase": "train",
        "loss": 3.0173022747039795,
        "learning_rate": 0.0005,
        "perplexity": 20.43608630622145
      },
      {
        "step": 5350,
        "timestamp": 1749085467.9593852,
        "phase": "train",
        "loss": 2.914083480834961,
        "learning_rate": 0.0005,
        "perplexity": 18.431911458810184
      },
      {
        "step": 5400,
        "timestamp": 1749085500.0349066,
        "phase": "train",
        "loss": 2.993316173553467,
        "learning_rate": 0.0005,
        "perplexity": 19.95173632839643
      },
      {
        "step": 5450,
        "timestamp": 1749085532.4994016,
        "phase": "train",
        "loss": 2.9690282344818115,
        "learning_rate": 0.0005,
        "perplexity": 19.47298722108883
      },
      {
        "step": 5500,
        "timestamp": 1749085564.3604655,
        "phase": "train",
        "loss": 2.991032123565674,
        "learning_rate": 0.0005,
        "perplexity": 19.906217568629295
      },
      {
        "step": 5550,
        "timestamp": 1749085596.779631,
        "phase": "train",
        "loss": 2.968539237976074,
        "learning_rate": 0.0005,
        "perplexity": 19.463467326168825
      },
      {
        "step": 5600,
        "timestamp": 1749085629.070638,
        "phase": "train",
        "loss": 3.014256477355957,
        "learning_rate": 0.0005,
        "perplexity": 20.37393682415594
      },
      {
        "step": 5650,
        "timestamp": 1749085660.8761818,
        "phase": "train",
        "loss": 2.9484593868255615,
        "learning_rate": 0.0005,
        "perplexity": 19.076541506453367
      },
      {
        "step": 5700,
        "timestamp": 1749085692.856639,
        "phase": "train",
        "loss": 2.94728422164917,
        "learning_rate": 0.0005,
        "perplexity": 19.054136586508303
      },
      {
        "step": 5750,
        "timestamp": 1749085725.2459717,
        "phase": "train",
        "loss": 2.872645378112793,
        "learning_rate": 0.0005,
        "perplexity": 17.683736548361747
      },
      {
        "step": 5800,
        "timestamp": 1749085757.6382065,
        "phase": "train",
        "loss": 2.9571619033813477,
        "learning_rate": 0.0005,
        "perplexity": 19.243279894220866
      },
      {
        "step": 5850,
        "timestamp": 1749085789.465557,
        "phase": "train",
        "loss": 2.9871931076049805,
        "learning_rate": 0.0005,
        "perplexity": 19.829943783482744
      },
      {
        "step": 5900,
        "timestamp": 1749085821.6571603,
        "phase": "train",
        "loss": 2.9079437255859375,
        "learning_rate": 0.0005,
        "perplexity": 18.31909073391323
      },
      {
        "step": 5950,
        "timestamp": 1749085853.3739173,
        "phase": "train",
        "loss": 2.9220666885375977,
        "learning_rate": 0.0005,
        "perplexity": 18.579646150091918
      },
      {
        "step": 6000,
        "timestamp": 1749085885.330729,
        "phase": "train",
        "loss": 2.9020211696624756,
        "learning_rate": 0.0005,
        "perplexity": 18.21091554757189
      },
      {
        "step": 6050,
        "timestamp": 1749085917.4974844,
        "phase": "train",
        "loss": 2.9708261489868164,
        "learning_rate": 0.0005,
        "perplexity": 19.508029479322087
      },
      {
        "step": 6100,
        "timestamp": 1749085949.7887506,
        "phase": "train",
        "loss": 2.9456562995910645,
        "learning_rate": 0.0005,
        "perplexity": 19.02314317153763
      },
      {
        "step": 6150,
        "timestamp": 1749085981.7270503,
        "phase": "train",
        "loss": 2.9894139766693115,
        "learning_rate": 0.0005,
        "perplexity": 19.874032431614882
      },
      {
        "step": 6200,
        "timestamp": 1749086013.9674373,
        "phase": "train",
        "loss": 2.8659791946411133,
        "learning_rate": 0.0005,
        "perplexity": 17.566245559398283
      },
      {
        "step": 6250,
        "timestamp": 1749086046.2164145,
        "phase": "train",
        "loss": 2.9581286907196045,
        "learning_rate": 0.0005,
        "perplexity": 19.26189304960083
      },
      {
        "step": 6300,
        "timestamp": 1749086078.0153227,
        "phase": "train",
        "loss": 3.0276753902435303,
        "learning_rate": 0.0005,
        "perplexity": 20.64917547923481
      },
      {
        "step": 6350,
        "timestamp": 1749086110.309526,
        "phase": "train",
        "loss": 2.9648237228393555,
        "learning_rate": 0.0005,
        "perplexity": 19.391284699566683
      },
      {
        "step": 6400,
        "timestamp": 1749086142.4777367,
        "phase": "train",
        "loss": 2.958367109298706,
        "learning_rate": 0.0005,
        "perplexity": 19.266485990271956
      },
      {
        "step": 6450,
        "timestamp": 1749086174.5619876,
        "phase": "train",
        "loss": 2.8786368370056152,
        "learning_rate": 0.0005,
        "perplexity": 17.790005965483832
      },
      {
        "step": 6500,
        "timestamp": 1749086206.7706287,
        "phase": "train",
        "loss": 2.897643566131592,
        "learning_rate": 0.0005,
        "perplexity": 18.131369616675112
      },
      {
        "step": 6550,
        "timestamp": 1749086238.5821002,
        "phase": "train",
        "loss": 2.8778152465820312,
        "learning_rate": 0.0005,
        "perplexity": 17.775395869526385
      },
      {
        "step": 6600,
        "timestamp": 1749086271.057823,
        "phase": "train",
        "loss": 2.9824092388153076,
        "learning_rate": 0.0005,
        "perplexity": 19.735306481019084
      },
      {
        "step": 6650,
        "timestamp": 1749086302.9264,
        "phase": "train",
        "loss": 2.877532958984375,
        "learning_rate": 0.0005,
        "perplexity": 17.770378803890004
      },
      {
        "step": 6700,
        "timestamp": 1749086334.8734782,
        "phase": "train",
        "loss": 2.849350929260254,
        "learning_rate": 0.0005,
        "perplexity": 17.27656448804128
      },
      {
        "step": 6750,
        "timestamp": 1749086366.4132223,
        "phase": "train",
        "loss": 2.853659152984619,
        "learning_rate": 0.0005,
        "perplexity": 17.351156356900074
      },
      {
        "step": 6800,
        "timestamp": 1749086398.5995288,
        "phase": "train",
        "loss": 2.9452109336853027,
        "learning_rate": 0.0005,
        "perplexity": 19.014672798496292
      },
      {
        "step": 6850,
        "timestamp": 1749086430.3451312,
        "phase": "train",
        "loss": 2.9070515632629395,
        "learning_rate": 0.0005,
        "perplexity": 18.302754419774416
      },
      {
        "step": 6900,
        "timestamp": 1749086462.1658754,
        "phase": "train",
        "loss": 2.918245553970337,
        "learning_rate": 0.0005,
        "perplexity": 18.50878629068855
      },
      {
        "step": 6950,
        "timestamp": 1749086494.355652,
        "phase": "train",
        "loss": 3.0176358222961426,
        "learning_rate": 0.0005,
        "perplexity": 20.442903850526665
      },
      {
        "step": 7000,
        "timestamp": 1749086526.4080775,
        "phase": "train",
        "loss": 2.975527286529541,
        "learning_rate": 0.0005,
        "perplexity": 19.599955317795093
      },
      {
        "step": 7050,
        "timestamp": 1749086558.2633076,
        "phase": "train",
        "loss": 2.9577770233154297,
        "learning_rate": 0.0005,
        "perplexity": 19.255120460591776
      },
      {
        "step": 7100,
        "timestamp": 1749086590.5613635,
        "phase": "train",
        "loss": 2.9133427143096924,
        "learning_rate": 0.0005,
        "perplexity": 18.418262771674183
      },
      {
        "step": 7150,
        "timestamp": 1749086622.4925296,
        "phase": "train",
        "loss": 2.9165844917297363,
        "learning_rate": 0.0005,
        "perplexity": 18.478067564582823
      },
      {
        "step": 7200,
        "timestamp": 1749086654.2529352,
        "phase": "train",
        "loss": 2.905776023864746,
        "learning_rate": 0.0005,
        "perplexity": 18.27942341838557
      },
      {
        "step": 7250,
        "timestamp": 1749086685.8421419,
        "phase": "train",
        "loss": 2.9231410026550293,
        "learning_rate": 0.0005,
        "perplexity": 18.59961725194537
      },
      {
        "step": 7300,
        "timestamp": 1749086717.7592793,
        "phase": "train",
        "loss": 3.0303702354431152,
        "learning_rate": 0.0005,
        "perplexity": 20.704896857172738
      },
      {
        "step": 7350,
        "timestamp": 1749086749.5071836,
        "phase": "train",
        "loss": 2.8955183029174805,
        "learning_rate": 0.0005,
        "perplexity": 18.09287660218977
      },
      {
        "step": 7400,
        "timestamp": 1749086782.0003855,
        "phase": "train",
        "loss": 3.0384392738342285,
        "learning_rate": 0.0005,
        "perplexity": 20.872641322920444
      },
      {
        "step": 7450,
        "timestamp": 1749086814.3773334,
        "phase": "train",
        "loss": 2.9390435218811035,
        "learning_rate": 0.0005,
        "perplexity": 18.897762368983656
      },
      {
        "step": 7500,
        "timestamp": 1749086846.7130606,
        "phase": "train",
        "loss": 2.937854290008545,
        "learning_rate": 0.0005,
        "perplexity": 18.875301905650957
      },
      {
        "step": 7550,
        "timestamp": 1749086878.454387,
        "phase": "train",
        "loss": 2.829362154006958,
        "learning_rate": 0.0005,
        "perplexity": 16.93465567663032
      },
      {
        "step": 7600,
        "timestamp": 1749086910.287124,
        "phase": "train",
        "loss": 2.9355316162109375,
        "learning_rate": 0.0005,
        "perplexity": 18.831511611463924
      },
      {
        "step": 7650,
        "timestamp": 1749086942.1561215,
        "phase": "train",
        "loss": 2.850647449493408,
        "learning_rate": 0.0005,
        "perplexity": 17.298978430384544
      },
      {
        "step": 7700,
        "timestamp": 1749086974.2249892,
        "phase": "train",
        "loss": 2.9510903358459473,
        "learning_rate": 0.0005,
        "perplexity": 19.126796995468325
      },
      {
        "step": 7750,
        "timestamp": 1749087006.3657033,
        "phase": "train",
        "loss": 2.907215118408203,
        "learning_rate": 0.0005,
        "perplexity": 18.30574817424757
      },
      {
        "step": 7800,
        "timestamp": 1749087038.100323,
        "phase": "train",
        "loss": 2.768789529800415,
        "learning_rate": 0.0005,
        "perplexity": 15.939328245782393
      },
      {
        "step": 7850,
        "timestamp": 1749087069.7984118,
        "phase": "train",
        "loss": 2.915658473968506,
        "learning_rate": 0.0005,
        "perplexity": 18.460964465933536
      },
      {
        "step": 7900,
        "timestamp": 1749087101.7766767,
        "phase": "train",
        "loss": 2.851177215576172,
        "learning_rate": 0.0005,
        "perplexity": 17.30814527034948
      },
      {
        "step": 7950,
        "timestamp": 1749087133.8063545,
        "phase": "train",
        "loss": 2.928457260131836,
        "learning_rate": 0.0005,
        "perplexity": 18.698760909327053
      },
      {
        "step": 8000,
        "timestamp": 1749087166.2232246,
        "phase": "train",
        "loss": 2.834751605987549,
        "learning_rate": 0.0005,
        "perplexity": 17.026170576272747
      },
      {
        "step": 8050,
        "timestamp": 1749087198.1459253,
        "phase": "train",
        "loss": 2.906850576400757,
        "learning_rate": 0.0005,
        "perplexity": 18.299076176245983
      },
      {
        "step": 8100,
        "timestamp": 1749087230.043706,
        "phase": "train",
        "loss": 2.854322910308838,
        "learning_rate": 0.0005,
        "perplexity": 17.36267713709377
      },
      {
        "step": 8150,
        "timestamp": 1749087262.2115653,
        "phase": "train",
        "loss": 2.8910722732543945,
        "learning_rate": 0.0005,
        "perplexity": 18.012613693975496
      },
      {
        "step": 8200,
        "timestamp": 1749087294.030778,
        "phase": "train",
        "loss": 2.9166486263275146,
        "learning_rate": 0.0005,
        "perplexity": 18.479252686017034
      },
      {
        "step": 8250,
        "timestamp": 1749087325.980723,
        "phase": "train",
        "loss": 2.838732957839966,
        "learning_rate": 0.0005,
        "perplexity": 17.094092873597706
      },
      {
        "step": 8300,
        "timestamp": 1749087357.920927,
        "phase": "train",
        "loss": 2.7859630584716797,
        "learning_rate": 0.0005,
        "perplexity": 16.215426763537447
      },
      {
        "step": 8350,
        "timestamp": 1749087389.5911193,
        "phase": "train",
        "loss": 2.826700448989868,
        "learning_rate": 0.0005,
        "perplexity": 16.889640553718845
      },
      {
        "step": 8400,
        "timestamp": 1749087421.759943,
        "phase": "train",
        "loss": 2.8214101791381836,
        "learning_rate": 0.0005,
        "perplexity": 16.80052572607236
      },
      {
        "step": 8450,
        "timestamp": 1749087454.0220907,
        "phase": "train",
        "loss": 2.9143218994140625,
        "learning_rate": 0.0005,
        "perplexity": 18.43630649285839
      },
      {
        "step": 8500,
        "timestamp": 1749087486.0008092,
        "phase": "train",
        "loss": 2.874983310699463,
        "learning_rate": 0.0005,
        "perplexity": 17.725128298999874
      },
      {
        "step": 8550,
        "timestamp": 1749087517.948308,
        "phase": "train",
        "loss": 2.8014612197875977,
        "learning_rate": 0.0005,
        "perplexity": 16.46869357891608
      },
      {
        "step": 8600,
        "timestamp": 1749087550.1308768,
        "phase": "train",
        "loss": 2.885730266571045,
        "learning_rate": 0.0005,
        "perplexity": 17.91664674749001
      },
      {
        "step": 8650,
        "timestamp": 1749087582.4012594,
        "phase": "train",
        "loss": 2.9267430305480957,
        "learning_rate": 0.0005,
        "perplexity": 18.66673439843593
      },
      {
        "step": 8700,
        "timestamp": 1749087614.3961422,
        "phase": "train",
        "loss": 2.8409414291381836,
        "learning_rate": 0.0005,
        "perplexity": 17.131886404681435
      },
      {
        "step": 8750,
        "timestamp": 1749087646.3577473,
        "phase": "train",
        "loss": 2.760558605194092,
        "learning_rate": 0.0005,
        "perplexity": 15.808671288145822
      },
      {
        "step": 8800,
        "timestamp": 1749087678.3086498,
        "phase": "train",
        "loss": 2.7994227409362793,
        "learning_rate": 0.0005,
        "perplexity": 16.43515668908021
      },
      {
        "step": 8850,
        "timestamp": 1749087710.5617385,
        "phase": "train",
        "loss": 2.829751968383789,
        "learning_rate": 0.0005,
        "perplexity": 16.94125833570186
      },
      {
        "step": 8900,
        "timestamp": 1749087742.4735763,
        "phase": "train",
        "loss": 2.8125925064086914,
        "learning_rate": 0.0005,
        "perplexity": 16.653035404857814
      },
      {
        "step": 8950,
        "timestamp": 1749087774.8513598,
        "phase": "train",
        "loss": 2.887504816055298,
        "learning_rate": 0.0005,
        "perplexity": 17.948468950421464
      },
      {
        "step": 9000,
        "timestamp": 1749087807.4606686,
        "phase": "train",
        "loss": 2.9016363620758057,
        "learning_rate": 0.0005,
        "perplexity": 18.203909197243817
      },
      {
        "step": 9050,
        "timestamp": 1749087839.5775788,
        "phase": "train",
        "loss": 2.835872173309326,
        "learning_rate": 0.0005,
        "perplexity": 17.045260240264824
      },
      {
        "step": 9100,
        "timestamp": 1749087871.681643,
        "phase": "train",
        "loss": 2.8400955200195312,
        "learning_rate": 0.0005,
        "perplexity": 17.11740051348952
      },
      {
        "step": 9150,
        "timestamp": 1749087903.3953469,
        "phase": "train",
        "loss": 2.822054862976074,
        "learning_rate": 0.0005,
        "perplexity": 16.811360245520564
      },
      {
        "step": 9200,
        "timestamp": 1749087935.276094,
        "phase": "train",
        "loss": 2.804436206817627,
        "learning_rate": 0.0005,
        "perplexity": 16.51776067951961
      },
      {
        "step": 9250,
        "timestamp": 1749087967.299401,
        "phase": "train",
        "loss": 2.8228611946105957,
        "learning_rate": 0.0005,
        "perplexity": 16.824921243702033
      },
      {
        "step": 9300,
        "timestamp": 1749087999.962691,
        "phase": "train",
        "loss": 2.8341188430786133,
        "learning_rate": 0.0005,
        "perplexity": 17.015400454876897
      },
      {
        "step": 9350,
        "timestamp": 1749088032.6649737,
        "phase": "train",
        "loss": 2.8345279693603516,
        "learning_rate": 0.0005,
        "perplexity": 17.022363326647085
      },
      {
        "step": 9400,
        "timestamp": 1749088064.2784793,
        "phase": "train",
        "loss": 2.811525821685791,
        "learning_rate": 0.0005,
        "perplexity": 16.635281337081373
      },
      {
        "step": 9450,
        "timestamp": 1749088096.6521316,
        "phase": "train",
        "loss": 2.863044500350952,
        "learning_rate": 0.0005,
        "perplexity": 17.51476956894762
      },
      {
        "step": 9500,
        "timestamp": 1749088128.7628715,
        "phase": "train",
        "loss": 2.9138941764831543,
        "learning_rate": 0.0005,
        "perplexity": 18.428422548002334
      },
      {
        "step": 9550,
        "timestamp": 1749088160.4035137,
        "phase": "train",
        "loss": 2.770612955093384,
        "learning_rate": 0.0005,
        "perplexity": 15.96841893434709
      },
      {
        "step": 9600,
        "timestamp": 1749088192.4531689,
        "phase": "train",
        "loss": 2.829759359359741,
        "learning_rate": 0.0005,
        "perplexity": 16.941383548597543
      },
      {
        "step": 9650,
        "timestamp": 1749088224.5446968,
        "phase": "train",
        "loss": 2.90208101272583,
        "learning_rate": 0.0005,
        "perplexity": 18.21200537715379
      },
      {
        "step": 9700,
        "timestamp": 1749088256.641698,
        "phase": "train",
        "loss": 2.927114963531494,
        "learning_rate": 0.0005,
        "perplexity": 18.673678463934497
      },
      {
        "step": 9750,
        "timestamp": 1749088291.898814,
        "phase": "train",
        "loss": 2.7771730422973633,
        "learning_rate": 0.0005,
        "perplexity": 16.073517506056557
      },
      {
        "step": 9800,
        "timestamp": 1749088323.762216,
        "phase": "train",
        "loss": 2.77199649810791,
        "learning_rate": 0.0005,
        "perplexity": 15.990527219171629
      },
      {
        "step": 9850,
        "timestamp": 1749088355.840548,
        "phase": "train",
        "loss": 2.7862155437469482,
        "learning_rate": 0.0005,
        "perplexity": 16.219521436928055
      },
      {
        "step": 9900,
        "timestamp": 1749088387.7499254,
        "phase": "train",
        "loss": 2.8365275859832764,
        "learning_rate": 0.0005,
        "perplexity": 17.05643558168723
      },
      {
        "step": 9950,
        "timestamp": 1749088420.3849704,
        "phase": "train",
        "loss": 2.7858493328094482,
        "learning_rate": 0.0005,
        "perplexity": 16.21358275824775
      },
      {
        "step": 10000,
        "timestamp": 1749088452.945336,
        "phase": "train",
        "loss": 2.767019748687744,
        "learning_rate": 0.0005,
        "perplexity": 15.911144070970838
      },
      {
        "step": 10050,
        "timestamp": 1749088485.1373985,
        "phase": "train",
        "loss": 2.8088865280151367,
        "learning_rate": 0.0005,
        "perplexity": 16.591433833011845
      },
      {
        "step": 10100,
        "timestamp": 1749088517.0112963,
        "phase": "train",
        "loss": 2.8289358615875244,
        "learning_rate": 0.0005,
        "perplexity": 16.9274380997981
      },
      {
        "step": 10150,
        "timestamp": 1749088549.055287,
        "phase": "train",
        "loss": 2.914527654647827,
        "learning_rate": 0.0005,
        "perplexity": 18.44010024968986
      },
      {
        "step": 10200,
        "timestamp": 1749088580.8156562,
        "phase": "train",
        "loss": 2.805514335632324,
        "learning_rate": 0.0005,
        "perplexity": 16.53557855651988
      },
      {
        "step": 10250,
        "timestamp": 1749088613.7134113,
        "phase": "train",
        "loss": 2.7360920906066895,
        "learning_rate": 0.0005,
        "perplexity": 15.426581464640183
      },
      {
        "step": 10300,
        "timestamp": 1749088645.5373082,
        "phase": "train",
        "loss": 2.798060417175293,
        "learning_rate": 0.0005,
        "perplexity": 16.412781928901353
      },
      {
        "step": 10350,
        "timestamp": 1749088677.2358658,
        "phase": "train",
        "loss": 2.82167911529541,
        "learning_rate": 0.0005,
        "perplexity": 16.805044602517913
      },
      {
        "step": 10400,
        "timestamp": 1749088709.1253414,
        "phase": "train",
        "loss": 2.838009834289551,
        "learning_rate": 0.0005,
        "perplexity": 17.08173620070686
      },
      {
        "step": 10450,
        "timestamp": 1749088741.401529,
        "phase": "train",
        "loss": 2.838768482208252,
        "learning_rate": 0.0005,
        "perplexity": 17.0947001412348
      },
      {
        "step": 10500,
        "timestamp": 1749088773.4802518,
        "phase": "train",
        "loss": 2.7709403038024902,
        "learning_rate": 0.0005,
        "perplexity": 15.973647031330433
      },
      {
        "step": 10550,
        "timestamp": 1749088805.407666,
        "phase": "train",
        "loss": 2.7166614532470703,
        "learning_rate": 0.0005,
        "perplexity": 15.129726534481563
      },
      {
        "step": 10600,
        "timestamp": 1749088837.021879,
        "phase": "train",
        "loss": 2.784219264984131,
        "learning_rate": 0.0005,
        "perplexity": 16.187175047687443
      },
      {
        "step": 10650,
        "timestamp": 1749088868.3471746,
        "phase": "train",
        "loss": 2.78115177154541,
        "learning_rate": 0.0005,
        "perplexity": 16.137597073368095
      },
      {
        "step": 10700,
        "timestamp": 1749088900.18543,
        "phase": "train",
        "loss": 2.8436315059661865,
        "learning_rate": 0.0005,
        "perplexity": 17.178034538502075
      },
      {
        "step": 10750,
        "timestamp": 1749088931.9280088,
        "phase": "train",
        "loss": 2.7855916023254395,
        "learning_rate": 0.0005,
        "perplexity": 16.209404562163325
      },
      {
        "step": 10800,
        "timestamp": 1749088964.1452918,
        "phase": "train",
        "loss": 2.8722472190856934,
        "learning_rate": 0.0005,
        "perplexity": 17.67669701054293
      },
      {
        "step": 10850,
        "timestamp": 1749088996.1715875,
        "phase": "train",
        "loss": 2.760502576828003,
        "learning_rate": 0.0005,
        "perplexity": 15.807785578936162
      },
      {
        "step": 10900,
        "timestamp": 1749089027.8812327,
        "phase": "train",
        "loss": 2.74101185798645,
        "learning_rate": 0.0005,
        "perplexity": 15.502663656796292
      },
      {
        "step": 10950,
        "timestamp": 1749089059.6302056,
        "phase": "train",
        "loss": 2.756910562515259,
        "learning_rate": 0.0005,
        "perplexity": 15.751105645393139
      },
      {
        "step": 11000,
        "timestamp": 1749089091.932708,
        "phase": "train",
        "loss": 2.8359720706939697,
        "learning_rate": 0.0005,
        "perplexity": 17.046963102237708
      },
      {
        "step": 11050,
        "timestamp": 1749089123.7694044,
        "phase": "train",
        "loss": 2.8660430908203125,
        "learning_rate": 0.0005,
        "perplexity": 17.567368011232215
      },
      {
        "step": 11100,
        "timestamp": 1749089156.1289606,
        "phase": "train",
        "loss": 2.8750290870666504,
        "learning_rate": 0.0005,
        "perplexity": 17.725939709552904
      },
      {
        "step": 11150,
        "timestamp": 1749089188.0172741,
        "phase": "train",
        "loss": 2.839028835296631,
        "learning_rate": 0.0005,
        "perplexity": 17.09915137863304
      },
      {
        "step": 11200,
        "timestamp": 1749089220.0747056,
        "phase": "train",
        "loss": 2.8036892414093018,
        "learning_rate": 0.0005,
        "perplexity": 16.505427090624618
      },
      {
        "step": 11250,
        "timestamp": 1749089252.3786776,
        "phase": "train",
        "loss": 2.8309879302978516,
        "learning_rate": 0.0005,
        "perplexity": 16.962210030862554
      },
      {
        "step": 11300,
        "timestamp": 1749089284.4558568,
        "phase": "train",
        "loss": 2.7875185012817383,
        "learning_rate": 0.0005,
        "perplexity": 16.2406685585039
      },
      {
        "step": 11350,
        "timestamp": 1749089316.5465362,
        "phase": "train",
        "loss": 2.817206859588623,
        "learning_rate": 0.0005,
        "perplexity": 16.73005595508244
      },
      {
        "step": 11400,
        "timestamp": 1749089348.6090634,
        "phase": "train",
        "loss": 2.8203418254852295,
        "learning_rate": 0.0005,
        "perplexity": 16.782586407515915
      },
      {
        "step": 11450,
        "timestamp": 1749089380.819801,
        "phase": "train",
        "loss": 2.7629294395446777,
        "learning_rate": 0.0005,
        "perplexity": 15.846195493333688
      },
      {
        "step": 11500,
        "timestamp": 1749089412.8544598,
        "phase": "train",
        "loss": 2.7379884719848633,
        "learning_rate": 0.0005,
        "perplexity": 15.455863903018422
      },
      {
        "step": 11550,
        "timestamp": 1749089444.5924966,
        "phase": "train",
        "loss": 2.8370792865753174,
        "learning_rate": 0.0005,
        "perplexity": 17.06584822353705
      },
      {
        "step": 11600,
        "timestamp": 1749089476.3313959,
        "phase": "train",
        "loss": 2.8581838607788086,
        "learning_rate": 0.0005,
        "perplexity": 17.42984315243882
      },
      {
        "step": 11650,
        "timestamp": 1749089507.8071578,
        "phase": "train",
        "loss": 2.7932608127593994,
        "learning_rate": 0.0005,
        "perplexity": 16.334195810279702
      },
      {
        "step": 11700,
        "timestamp": 1749089540.2262204,
        "phase": "train",
        "loss": 2.7890799045562744,
        "learning_rate": 0.0005,
        "perplexity": 16.266046599093713
      },
      {
        "step": 11750,
        "timestamp": 1749089572.2040362,
        "phase": "train",
        "loss": 2.7128119468688965,
        "learning_rate": 0.0005,
        "perplexity": 15.071596513414738
      },
      {
        "step": 11800,
        "timestamp": 1749089603.6826143,
        "phase": "train",
        "loss": 2.7611167430877686,
        "learning_rate": 0.0005,
        "perplexity": 15.817497169440696
      },
      {
        "step": 11850,
        "timestamp": 1749089635.7098174,
        "phase": "train",
        "loss": 2.6833136081695557,
        "learning_rate": 0.0005,
        "perplexity": 14.63350274085129
      },
      {
        "step": 11900,
        "timestamp": 1749089667.8208694,
        "phase": "train",
        "loss": 2.8340954780578613,
        "learning_rate": 0.0005,
        "perplexity": 17.01500289433669
      },
      {
        "step": 11950,
        "timestamp": 1749089699.7066915,
        "phase": "train",
        "loss": 2.7881622314453125,
        "learning_rate": 0.0005,
        "perplexity": 16.251126532427115
      },
      {
        "step": 12000,
        "timestamp": 1749089731.3944376,
        "phase": "train",
        "loss": 2.835174083709717,
        "learning_rate": 0.0005,
        "perplexity": 17.03336527372774
      },
      {
        "step": 12050,
        "timestamp": 1749089763.2701244,
        "phase": "train",
        "loss": 2.793081760406494,
        "learning_rate": 0.0005,
        "perplexity": 16.331271395906505
      },
      {
        "step": 12100,
        "timestamp": 1749089795.3330355,
        "phase": "train",
        "loss": 2.7770793437957764,
        "learning_rate": 0.0005,
        "perplexity": 16.072011512106798
      },
      {
        "step": 12150,
        "timestamp": 1749089826.963148,
        "phase": "train",
        "loss": 2.734576463699341,
        "learning_rate": 0.0005,
        "perplexity": 15.403218232128243
      },
      {
        "step": 12200,
        "timestamp": 1749089858.6971295,
        "phase": "train",
        "loss": 2.763019561767578,
        "learning_rate": 0.0005,
        "perplexity": 15.847623652049515
      },
      {
        "step": 12250,
        "timestamp": 1749089890.4791496,
        "phase": "train",
        "loss": 2.848067283630371,
        "learning_rate": 0.0005,
        "perplexity": 17.254401729144337
      },
      {
        "step": 12300,
        "timestamp": 1749089922.5441337,
        "phase": "train",
        "loss": 2.8157167434692383,
        "learning_rate": 0.0005,
        "perplexity": 16.705144793896473
      },
      {
        "step": 12350,
        "timestamp": 1749089954.9662244,
        "phase": "train",
        "loss": 2.843984603881836,
        "learning_rate": 0.0005,
        "perplexity": 17.184101137681306
      },
      {
        "step": 12400,
        "timestamp": 1749089986.9418511,
        "phase": "train",
        "loss": 2.788435935974121,
        "learning_rate": 0.0005,
        "perplexity": 16.255575148132642
      },
      {
        "step": 12450,
        "timestamp": 1749090019.5558643,
        "phase": "train",
        "loss": 2.8784947395324707,
        "learning_rate": 0.0005,
        "perplexity": 17.787478230185563
      },
      {
        "step": 12500,
        "timestamp": 1749090051.5326872,
        "phase": "train",
        "loss": 2.84853196144104,
        "learning_rate": 0.0005,
        "perplexity": 17.262421329885193
      },
      {
        "step": 12550,
        "timestamp": 1749090083.8080978,
        "phase": "train",
        "loss": 2.744703769683838,
        "learning_rate": 0.0005,
        "perplexity": 15.560003904527187
      },
      {
        "step": 12600,
        "timestamp": 1749090116.1806328,
        "phase": "train",
        "loss": 2.7532668113708496,
        "learning_rate": 0.0005,
        "perplexity": 15.693816972390692
      },
      {
        "step": 12650,
        "timestamp": 1749090148.1742947,
        "phase": "train",
        "loss": 2.7309114933013916,
        "learning_rate": 0.0005,
        "perplexity": 15.3468692150874
      },
      {
        "step": 12700,
        "timestamp": 1749090179.6468914,
        "phase": "train",
        "loss": 2.7176122665405273,
        "learning_rate": 0.0005,
        "perplexity": 15.14411892074873
      },
      {
        "step": 12750,
        "timestamp": 1749090211.5336254,
        "phase": "train",
        "loss": 2.8039910793304443,
        "learning_rate": 0.0005,
        "perplexity": 16.510409806373673
      },
      {
        "step": 12800,
        "timestamp": 1749090243.582719,
        "phase": "train",
        "loss": 2.8267011642456055,
        "learning_rate": 0.0005,
        "perplexity": 16.889652634135473
      },
      {
        "step": 12850,
        "timestamp": 1749090275.5256953,
        "phase": "train",
        "loss": 2.778214931488037,
        "learning_rate": 0.0005,
        "perplexity": 16.090273057400413
      },
      {
        "step": 12900,
        "timestamp": 1749090307.6290913,
        "phase": "train",
        "loss": 2.7581019401550293,
        "learning_rate": 0.0005,
        "perplexity": 15.769882343308806
      },
      {
        "step": 12950,
        "timestamp": 1749090339.3831642,
        "phase": "train",
        "loss": 2.7097103595733643,
        "learning_rate": 0.0005,
        "perplexity": 15.024923059457198
      },
      {
        "step": 13000,
        "timestamp": 1749090371.9891038,
        "phase": "train",
        "loss": 2.715538501739502,
        "learning_rate": 0.0005,
        "perplexity": 15.11274612113534
      },
      {
        "step": 13050,
        "timestamp": 1749090404.28823,
        "phase": "train",
        "loss": 2.794588565826416,
        "learning_rate": 0.0005,
        "perplexity": 16.355897993245765
      },
      {
        "step": 13100,
        "timestamp": 1749090436.1426587,
        "phase": "train",
        "loss": 2.8238742351531982,
        "learning_rate": 0.0005,
        "perplexity": 16.84197420726134
      },
      {
        "step": 13150,
        "timestamp": 1749090468.0970323,
        "phase": "train",
        "loss": 2.6918411254882812,
        "learning_rate": 0.0005,
        "perplexity": 14.758823768093242
      },
      {
        "step": 13200,
        "timestamp": 1749090500.2347245,
        "phase": "train",
        "loss": 2.834456443786621,
        "learning_rate": 0.0005,
        "perplexity": 17.021145835885278
      },
      {
        "step": 13250,
        "timestamp": 1749090532.1875699,
        "phase": "train",
        "loss": 2.8216285705566406,
        "learning_rate": 0.0005,
        "perplexity": 16.804195217394625
      },
      {
        "step": 13300,
        "timestamp": 1749090564.3606234,
        "phase": "train",
        "loss": 2.7027554512023926,
        "learning_rate": 0.0005,
        "perplexity": 14.920788638729654
      },
      {
        "step": 13350,
        "timestamp": 1749090596.5232177,
        "phase": "train",
        "loss": 2.7981626987457275,
        "learning_rate": 0.0005,
        "perplexity": 16.414460739866488
      },
      {
        "step": 13400,
        "timestamp": 1749090628.7842295,
        "phase": "train",
        "loss": 2.8517050743103027,
        "learning_rate": 0.0005,
        "perplexity": 17.317283937752546
      },
      {
        "step": 13450,
        "timestamp": 1749090661.1570172,
        "phase": "train",
        "loss": 2.8222222328186035,
        "learning_rate": 0.0005,
        "perplexity": 16.81417419571619
      },
      {
        "step": 13500,
        "timestamp": 1749090693.3185885,
        "phase": "train",
        "loss": 2.764399766921997,
        "learning_rate": 0.0005,
        "perplexity": 15.869511725440551
      },
      {
        "step": 13550,
        "timestamp": 1749090725.348638,
        "phase": "train",
        "loss": 2.7342529296875,
        "learning_rate": 0.0005,
        "perplexity": 15.39823557321162
      },
      {
        "step": 13600,
        "timestamp": 1749090757.5155675,
        "phase": "train",
        "loss": 2.708024024963379,
        "learning_rate": 0.0005,
        "perplexity": 14.999607363056414
      },
      {
        "step": 13650,
        "timestamp": 1749090789.7742155,
        "phase": "train",
        "loss": 2.7125656604766846,
        "learning_rate": 0.0005,
        "perplexity": 15.067885041345868
      },
      {
        "step": 13700,
        "timestamp": 1749090821.3868973,
        "phase": "train",
        "loss": 2.6840977668762207,
        "learning_rate": 0.0005,
        "perplexity": 14.644982229716874
      },
      {
        "step": 13750,
        "timestamp": 1749090853.6640804,
        "phase": "train",
        "loss": 2.850910186767578,
        "learning_rate": 0.0005,
        "perplexity": 17.303524113957373
      },
      {
        "step": 13800,
        "timestamp": 1749090886.0990279,
        "phase": "train",
        "loss": 2.813206195831299,
        "learning_rate": 0.0005,
        "perplexity": 16.663258333070697
      },
      {
        "step": 13850,
        "timestamp": 1749090918.1555321,
        "phase": "train",
        "loss": 2.7440671920776367,
        "learning_rate": 0.0005,
        "perplexity": 15.55010190651864
      },
      {
        "step": 13900,
        "timestamp": 1749090950.0407014,
        "phase": "train",
        "loss": 2.791609048843384,
        "learning_rate": 0.0005,
        "perplexity": 16.307237845269245
      },
      {
        "step": 13950,
        "timestamp": 1749090982.0750632,
        "phase": "train",
        "loss": 2.7286500930786133,
        "learning_rate": 0.0005,
        "perplexity": 15.312203013476854
      },
      {
        "step": 14000,
        "timestamp": 1749091014.4728503,
        "phase": "train",
        "loss": 2.8193891048431396,
        "learning_rate": 0.0005,
        "perplexity": 16.766604905180237
      },
      {
        "step": 14050,
        "timestamp": 1749091046.680201,
        "phase": "train",
        "loss": 2.758103132247925,
        "learning_rate": 0.0005,
        "perplexity": 15.769901142484715
      },
      {
        "step": 14100,
        "timestamp": 1749091078.863829,
        "phase": "train",
        "loss": 2.7303307056427,
        "learning_rate": 0.0005,
        "perplexity": 15.337958530705961
      },
      {
        "step": 14150,
        "timestamp": 1749091110.9281833,
        "phase": "train",
        "loss": 2.7929623126983643,
        "learning_rate": 0.0005,
        "perplexity": 16.32932077946807
      },
      {
        "step": 14200,
        "timestamp": 1749091143.045356,
        "phase": "train",
        "loss": 2.658989667892456,
        "learning_rate": 0.0005,
        "perplexity": 14.281852392944188
      },
      {
        "step": 14250,
        "timestamp": 1749091174.9989448,
        "phase": "train",
        "loss": 2.8558878898620605,
        "learning_rate": 0.0005,
        "perplexity": 17.389870644894266
      },
      {
        "step": 14300,
        "timestamp": 1749091207.2014248,
        "phase": "train",
        "loss": 2.801720142364502,
        "learning_rate": 0.0005,
        "perplexity": 16.472958247581456
      },
      {
        "step": 14350,
        "timestamp": 1749091239.6409662,
        "phase": "train",
        "loss": 2.8011674880981445,
        "learning_rate": 0.0005,
        "perplexity": 16.463856912103992
      },
      {
        "step": 14400,
        "timestamp": 1749091271.5585175,
        "phase": "train",
        "loss": 2.795107364654541,
        "learning_rate": 0.0005,
        "perplexity": 16.364385615451447
      },
      {
        "step": 14450,
        "timestamp": 1749091303.5073903,
        "phase": "train",
        "loss": 2.7180113792419434,
        "learning_rate": 0.0005,
        "perplexity": 15.150164337282769
      },
      {
        "step": 14500,
        "timestamp": 1749091335.4358497,
        "phase": "train",
        "loss": 2.766223192214966,
        "learning_rate": 0.0005,
        "perplexity": 15.898474992659844
      },
      {
        "step": 14550,
        "timestamp": 1749091367.1224554,
        "phase": "train",
        "loss": 2.746833562850952,
        "learning_rate": 0.0005,
        "perplexity": 15.593178809826467
      },
      {
        "step": 14600,
        "timestamp": 1749091398.9707587,
        "phase": "train",
        "loss": 2.691962957382202,
        "learning_rate": 0.0005,
        "perplexity": 14.76062197308209
      },
      {
        "step": 14650,
        "timestamp": 1749091431.14619,
        "phase": "train",
        "loss": 2.5825843811035156,
        "learning_rate": 0.0005,
        "perplexity": 13.231288704141887
      },
      {
        "step": 14700,
        "timestamp": 1749091462.8996453,
        "phase": "train",
        "loss": 2.5966882705688477,
        "learning_rate": 0.0005,
        "perplexity": 13.419223528183611
      },
      {
        "step": 14750,
        "timestamp": 1749091494.4655967,
        "phase": "train",
        "loss": 2.7425785064697266,
        "learning_rate": 0.0005,
        "perplexity": 15.526969916011607
      },
      {
        "step": 14800,
        "timestamp": 1749091527.0143883,
        "phase": "train",
        "loss": 2.804471254348755,
        "learning_rate": 0.0005,
        "perplexity": 16.518339596395936
      },
      {
        "step": 14850,
        "timestamp": 1749091559.0014296,
        "phase": "train",
        "loss": 2.7806193828582764,
        "learning_rate": 0.0005,
        "perplexity": 16.129007885844718
      },
      {
        "step": 14900,
        "timestamp": 1749091591.0070136,
        "phase": "train",
        "loss": 2.7489240169525146,
        "learning_rate": 0.0005,
        "perplexity": 15.625809729262466
      },
      {
        "step": 14950,
        "timestamp": 1749091622.63169,
        "phase": "train",
        "loss": 2.714895009994507,
        "learning_rate": 0.0005,
        "perplexity": 15.10302432204638
      },
      {
        "step": 15000,
        "timestamp": 1749091655.0616655,
        "phase": "train",
        "loss": 2.700868606567383,
        "learning_rate": 0.0005,
        "perplexity": 14.89266197240595
      },
      {
        "step": 15050,
        "timestamp": 1749091687.4825869,
        "phase": "train",
        "loss": 2.837462902069092,
        "learning_rate": 0.0005,
        "perplexity": 17.072396203202928
      },
      {
        "step": 15100,
        "timestamp": 1749091719.6129837,
        "phase": "train",
        "loss": 2.755049228668213,
        "learning_rate": 0.0005,
        "perplexity": 15.721814847760147
      },
      {
        "step": 15150,
        "timestamp": 1749091751.6370187,
        "phase": "train",
        "loss": 2.737255573272705,
        "learning_rate": 0.0005,
        "perplexity": 15.444540470240113
      },
      {
        "step": 15200,
        "timestamp": 1749091783.6147034,
        "phase": "train",
        "loss": 2.7898874282836914,
        "learning_rate": 0.0005,
        "perplexity": 16.279187122601943
      },
      {
        "step": 15250,
        "timestamp": 1749091815.9307375,
        "phase": "train",
        "loss": 2.733126640319824,
        "learning_rate": 0.0005,
        "perplexity": 15.38090246708341
      },
      {
        "step": 15300,
        "timestamp": 1749091847.341681,
        "phase": "train",
        "loss": 2.7510013580322266,
        "learning_rate": 0.0005,
        "perplexity": 15.658303604462528
      },
      {
        "step": 15350,
        "timestamp": 1749091879.4264684,
        "phase": "train",
        "loss": 2.7589128017425537,
        "learning_rate": 0.0005,
        "perplexity": 15.782674720863648
      },
      {
        "step": 15400,
        "timestamp": 1749091911.2558708,
        "phase": "train",
        "loss": 2.7260003089904785,
        "learning_rate": 0.0005,
        "perplexity": 15.271682690340164
      },
      {
        "step": 15450,
        "timestamp": 1749091942.8451495,
        "phase": "train",
        "loss": 2.681300640106201,
        "learning_rate": 0.0005,
        "perplexity": 14.60407559506794
      },
      {
        "step": 15500,
        "timestamp": 1749091975.1459239,
        "phase": "train",
        "loss": 2.8749585151672363,
        "learning_rate": 0.0005,
        "perplexity": 17.724688800458736
      },
      {
        "step": 15550,
        "timestamp": 1749092007.1781783,
        "phase": "train",
        "loss": 2.7636184692382812,
        "learning_rate": 0.0005,
        "perplexity": 15.857117755008426
      },
      {
        "step": 15600,
        "timestamp": 1749092039.1990886,
        "phase": "train",
        "loss": 2.770221471786499,
        "learning_rate": 0.0005,
        "perplexity": 15.962168788390679
      },
      {
        "step": 15650,
        "timestamp": 1749092071.0282254,
        "phase": "train",
        "loss": 2.738591432571411,
        "learning_rate": 0.0005,
        "perplexity": 15.465185989926054
      },
      {
        "step": 15700,
        "timestamp": 1749092102.449085,
        "phase": "train",
        "loss": 2.7346832752227783,
        "learning_rate": 0.0005,
        "perplexity": 15.404863561201944
      },
      {
        "step": 15750,
        "timestamp": 1749092134.1170702,
        "phase": "train",
        "loss": 2.7093639373779297,
        "learning_rate": 0.0005,
        "perplexity": 15.019718994078627
      },
      {
        "step": 15800,
        "timestamp": 1749092166.3118174,
        "phase": "train",
        "loss": 2.802983283996582,
        "learning_rate": 0.0005,
        "perplexity": 16.49377907400548
      },
      {
        "step": 15850,
        "timestamp": 1749092198.4118128,
        "phase": "train",
        "loss": 2.7544143199920654,
        "learning_rate": 0.0005,
        "perplexity": 15.711836099240735
      },
      {
        "step": 15900,
        "timestamp": 1749092230.1118784,
        "phase": "train",
        "loss": 2.686140298843384,
        "learning_rate": 0.0005,
        "perplexity": 14.674925643859734
      },
      {
        "step": 15950,
        "timestamp": 1749092262.1204681,
        "phase": "train",
        "loss": 2.796945571899414,
        "learning_rate": 0.0005,
        "perplexity": 16.394494412273744
      },
      {
        "step": 16000,
        "timestamp": 1749092293.6027467,
        "phase": "train",
        "loss": 2.712332010269165,
        "learning_rate": 0.0005,
        "perplexity": 15.064364838143197
      },
      {
        "step": 16050,
        "timestamp": 1749092325.2953918,
        "phase": "train",
        "loss": 2.7089345455169678,
        "learning_rate": 0.0005,
        "perplexity": 15.013271033438615
      },
      {
        "step": 16100,
        "timestamp": 1749092357.2676466,
        "phase": "train",
        "loss": 2.7126963138580322,
        "learning_rate": 0.0005,
        "perplexity": 15.069853840088584
      },
      {
        "step": 16150,
        "timestamp": 1749092389.5284297,
        "phase": "train",
        "loss": 2.7161624431610107,
        "learning_rate": 0.0005,
        "perplexity": 15.122178531762904
      },
      {
        "step": 16200,
        "timestamp": 1749092421.7893612,
        "phase": "train",
        "loss": 2.7029733657836914,
        "learning_rate": 0.0005,
        "perplexity": 14.924040450434235
      },
      {
        "step": 16250,
        "timestamp": 1749092453.6120627,
        "phase": "train",
        "loss": 2.7336082458496094,
        "learning_rate": 0.0005,
        "perplexity": 15.388311778804178
      },
      {
        "step": 16300,
        "timestamp": 1749092486.1663916,
        "phase": "train",
        "loss": 2.6826584339141846,
        "learning_rate": 0.0005,
        "perplexity": 14.62391838664349
      },
      {
        "step": 16350,
        "timestamp": 1749092518.2679603,
        "phase": "train",
        "loss": 2.732048273086548,
        "learning_rate": 0.0005,
        "perplexity": 15.364325145671247
      },
      {
        "step": 16400,
        "timestamp": 1749092550.2638173,
        "phase": "train",
        "loss": 2.847597599029541,
        "learning_rate": 0.0005,
        "perplexity": 17.246299505249468
      },
      {
        "step": 16450,
        "timestamp": 1749092582.0219507,
        "phase": "train",
        "loss": 2.6860668659210205,
        "learning_rate": 0.0005,
        "perplexity": 14.673848060749764
      },
      {
        "step": 16500,
        "timestamp": 1749092613.884179,
        "phase": "train",
        "loss": 2.710632085800171,
        "learning_rate": 0.0005,
        "perplexity": 15.03877830948959
      },
      {
        "step": 16550,
        "timestamp": 1749092645.7875025,
        "phase": "train",
        "loss": 2.73763370513916,
        "learning_rate": 0.0005,
        "perplexity": 15.450381647452687
      },
      {
        "step": 16600,
        "timestamp": 1749092677.7655842,
        "phase": "train",
        "loss": 2.7711474895477295,
        "learning_rate": 0.0005,
        "perplexity": 15.976956886160337
      },
      {
        "step": 16650,
        "timestamp": 1749092710.0413396,
        "phase": "train",
        "loss": 2.700679063796997,
        "learning_rate": 0.0005,
        "perplexity": 14.889839443500707
      },
      {
        "step": 16700,
        "timestamp": 1749092742.0892498,
        "phase": "train",
        "loss": 2.7742440700531006,
        "learning_rate": 0.0005,
        "perplexity": 16.026507498525035
      },
      {
        "step": 16750,
        "timestamp": 1749092773.964167,
        "phase": "train",
        "loss": 2.7737646102905273,
        "learning_rate": 0.0005,
        "perplexity": 16.01882527485065
      },
      {
        "step": 16800,
        "timestamp": 1749092806.6767838,
        "phase": "train",
        "loss": 2.768444299697876,
        "learning_rate": 0.0005,
        "perplexity": 15.933826459603445
      },
      {
        "step": 16850,
        "timestamp": 1749092838.260545,
        "phase": "train",
        "loss": 2.7932066917419434,
        "learning_rate": 0.0005,
        "perplexity": 16.333311810904814
      },
      {
        "step": 16900,
        "timestamp": 1749092870.3754258,
        "phase": "train",
        "loss": 2.7439708709716797,
        "learning_rate": 0.0005,
        "perplexity": 15.548604175637964
      },
      {
        "step": 16950,
        "timestamp": 1749092902.6005561,
        "phase": "train",
        "loss": 2.7913999557495117,
        "learning_rate": 0.0005,
        "perplexity": 16.30382847090641
      },
      {
        "step": 17000,
        "timestamp": 1749092934.6535218,
        "phase": "train",
        "loss": 2.8204503059387207,
        "learning_rate": 0.0005,
        "perplexity": 16.78440708885254
      },
      {
        "step": 17050,
        "timestamp": 1749092966.6279702,
        "phase": "train",
        "loss": 2.707852363586426,
        "learning_rate": 0.0005,
        "perplexity": 14.997032730791497
      },
      {
        "step": 17100,
        "timestamp": 1749092998.259048,
        "phase": "train",
        "loss": 2.7210545539855957,
        "learning_rate": 0.0005,
        "perplexity": 15.196339158145415
      },
      {
        "step": 17150,
        "timestamp": 1749093030.0874965,
        "phase": "train",
        "loss": 2.7847490310668945,
        "learning_rate": 0.0005,
        "perplexity": 16.195752735886995
      },
      {
        "step": 17200,
        "timestamp": 1749093062.2090483,
        "phase": "train",
        "loss": 2.8064541816711426,
        "learning_rate": 0.0005,
        "perplexity": 16.551126759839953
      },
      {
        "step": 17250,
        "timestamp": 1749093093.946525,
        "phase": "train",
        "loss": 2.7827229499816895,
        "learning_rate": 0.0005,
        "perplexity": 16.162972046988763
      },
      {
        "step": 17300,
        "timestamp": 1749093125.888085,
        "phase": "train",
        "loss": 2.763526678085327,
        "learning_rate": 0.0005,
        "perplexity": 15.855662278688111
      },
      {
        "step": 17350,
        "timestamp": 1749093157.6362667,
        "phase": "train",
        "loss": 2.83693265914917,
        "learning_rate": 0.0005,
        "perplexity": 17.063346085582516
      },
      {
        "step": 17400,
        "timestamp": 1749093189.5743413,
        "phase": "train",
        "loss": 2.75071120262146,
        "learning_rate": 0.0005,
        "perplexity": 15.653760922022084
      },
      {
        "step": 17450,
        "timestamp": 1749093222.3459766,
        "phase": "train",
        "loss": 2.716218948364258,
        "learning_rate": 0.0005,
        "perplexity": 15.123033037676167
      },
      {
        "step": 17500,
        "timestamp": 1749093254.2580593,
        "phase": "train",
        "loss": 2.6627984046936035,
        "learning_rate": 0.0005,
        "perplexity": 14.336351931056974
      },
      {
        "step": 17550,
        "timestamp": 1749093286.1608725,
        "phase": "train",
        "loss": 2.6942429542541504,
        "learning_rate": 0.0005,
        "perplexity": 14.79431453988661
      },
      {
        "step": 17600,
        "timestamp": 1749093318.1662974,
        "phase": "train",
        "loss": 2.6430978775024414,
        "learning_rate": 0.0005,
        "perplexity": 14.056682106687655
      },
      {
        "step": 17650,
        "timestamp": 1749093350.6007986,
        "phase": "train",
        "loss": 2.69081974029541,
        "learning_rate": 0.0005,
        "perplexity": 14.743757019818952
      },
      {
        "step": 17700,
        "timestamp": 1749093382.3877532,
        "phase": "train",
        "loss": 2.6675307750701904,
        "learning_rate": 0.0005,
        "perplexity": 14.404357645440443
      },
      {
        "step": 17750,
        "timestamp": 1749093414.4288468,
        "phase": "train",
        "loss": 2.736082077026367,
        "learning_rate": 0.0005,
        "perplexity": 15.42642699010101
      },
      {
        "step": 17800,
        "timestamp": 1749093446.6594067,
        "phase": "train",
        "loss": 2.770951271057129,
        "learning_rate": 0.0005,
        "perplexity": 15.973822219345598
      },
      {
        "step": 17850,
        "timestamp": 1749093478.472737,
        "phase": "train",
        "loss": 2.735283374786377,
        "learning_rate": 0.0005,
        "perplexity": 15.414110787453028
      },
      {
        "step": 17900,
        "timestamp": 1749093509.7468734,
        "phase": "train",
        "loss": 2.770442008972168,
        "learning_rate": 0.0005,
        "perplexity": 15.96568942837419
      },
      {
        "step": 17950,
        "timestamp": 1749093541.8077378,
        "phase": "train",
        "loss": 2.6981682777404785,
        "learning_rate": 0.0005,
        "perplexity": 14.852501136108154
      },
      {
        "step": 18000,
        "timestamp": 1749093574.180859,
        "phase": "train",
        "loss": 2.754603385925293,
        "learning_rate": 0.0005,
        "perplexity": 15.71480695303033
      },
      {
        "step": 18050,
        "timestamp": 1749093606.2037659,
        "phase": "train",
        "loss": 2.77685546875,
        "learning_rate": 0.0005,
        "perplexity": 16.068413792528652
      },
      {
        "step": 18100,
        "timestamp": 1749093637.9295118,
        "phase": "train",
        "loss": 2.7462329864501953,
        "learning_rate": 0.0005,
        "perplexity": 15.583816726225143
      },
      {
        "step": 18150,
        "timestamp": 1749093669.894545,
        "phase": "train",
        "loss": 2.6879405975341797,
        "learning_rate": 0.0005,
        "perplexity": 14.701368688831531
      },
      {
        "step": 18200,
        "timestamp": 1749093701.8980548,
        "phase": "train",
        "loss": 2.6700057983398438,
        "learning_rate": 0.0005,
        "perplexity": 14.440052920894336
      },
      {
        "step": 18250,
        "timestamp": 1749093733.280799,
        "phase": "train",
        "loss": 2.787930965423584,
        "learning_rate": 0.0005,
        "perplexity": 16.247368633599258
      },
      {
        "step": 18300,
        "timestamp": 1749093765.2793517,
        "phase": "train",
        "loss": 2.6277987957000732,
        "learning_rate": 0.0005,
        "perplexity": 13.843264486808156
      },
      {
        "step": 18350,
        "timestamp": 1749093797.4905329,
        "phase": "train",
        "loss": 2.7791643142700195,
        "learning_rate": 0.0005,
        "perplexity": 16.10555613919794
      },
      {
        "step": 18400,
        "timestamp": 1749093829.8680289,
        "phase": "train",
        "loss": 2.775460958480835,
        "learning_rate": 0.0005,
        "perplexity": 16.046021841017666
      },
      {
        "step": 18450,
        "timestamp": 1749093862.0866945,
        "phase": "train",
        "loss": 2.6639316082000732,
        "learning_rate": 0.0005,
        "perplexity": 14.352607143827777
      },
      {
        "step": 18500,
        "timestamp": 1749093893.8821197,
        "phase": "train",
        "loss": 2.748133420944214,
        "learning_rate": 0.0005,
        "perplexity": 15.613460908571426
      },
      {
        "step": 18550,
        "timestamp": 1749093925.8595533,
        "phase": "train",
        "loss": 2.7683091163635254,
        "learning_rate": 0.0005,
        "perplexity": 15.931672617398737
      },
      {
        "step": 18600,
        "timestamp": 1749093958.1301458,
        "phase": "train",
        "loss": 2.790243625640869,
        "learning_rate": 0.0005,
        "perplexity": 16.28498675887825
      },
      {
        "step": 18650,
        "timestamp": 1749093990.3765879,
        "phase": "train",
        "loss": 2.7065677642822266,
        "learning_rate": 0.0005,
        "perplexity": 14.977779921700627
      },
      {
        "step": 18700,
        "timestamp": 1749094022.0703723,
        "phase": "train",
        "loss": 2.710886240005493,
        "learning_rate": 0.0005,
        "perplexity": 15.042600963991138
      },
      {
        "step": 18750,
        "timestamp": 1749094054.5411487,
        "phase": "train",
        "loss": 2.7320847511291504,
        "learning_rate": 0.0005,
        "perplexity": 15.364885616400844
      },
      {
        "step": 18800,
        "timestamp": 1749094086.5547416,
        "phase": "train",
        "loss": 2.6925530433654785,
        "learning_rate": 0.0005,
        "perplexity": 14.769334579553513
      },
      {
        "step": 18850,
        "timestamp": 1749094118.994917,
        "phase": "train",
        "loss": 2.6549155712127686,
        "learning_rate": 0.0005,
        "perplexity": 14.223785111707134
      },
      {
        "step": 18900,
        "timestamp": 1749094151.2102785,
        "phase": "train",
        "loss": 2.708204507827759,
        "learning_rate": 0.0005,
        "perplexity": 15.002314779471659
      },
      {
        "step": 18950,
        "timestamp": 1749094184.1395102,
        "phase": "train",
        "loss": 2.6340909004211426,
        "learning_rate": 0.0005,
        "perplexity": 13.930642363738363
      },
      {
        "step": 19000,
        "timestamp": 1749094216.0863395,
        "phase": "train",
        "loss": 2.7272067070007324,
        "learning_rate": 0.0005,
        "perplexity": 15.290117535595504
      },
      {
        "step": 19050,
        "timestamp": 1749094247.921748,
        "phase": "train",
        "loss": 2.732064723968506,
        "learning_rate": 0.0005,
        "perplexity": 15.364577904449629
      },
      {
        "step": 19100,
        "timestamp": 1749094280.2657003,
        "phase": "train",
        "loss": 2.6798348426818848,
        "learning_rate": 0.0005,
        "perplexity": 14.582684659894987
      },
      {
        "step": 19150,
        "timestamp": 1749094312.0085783,
        "phase": "train",
        "loss": 2.6798958778381348,
        "learning_rate": 0.0005,
        "perplexity": 14.583574743494667
      },
      {
        "step": 19200,
        "timestamp": 1749094343.8053424,
        "phase": "train",
        "loss": 2.7603981494903564,
        "learning_rate": 0.0005,
        "perplexity": 15.806134900163563
      },
      {
        "step": 19250,
        "timestamp": 1749094375.7096207,
        "phase": "train",
        "loss": 2.712631940841675,
        "learning_rate": 0.0005,
        "perplexity": 15.068883779364034
      },
      {
        "step": 19300,
        "timestamp": 1749094407.5192618,
        "phase": "train",
        "loss": 2.7490577697753906,
        "learning_rate": 0.0005,
        "perplexity": 15.627899865201153
      },
      {
        "step": 19350,
        "timestamp": 1749094439.5011923,
        "phase": "train",
        "loss": 2.6905789375305176,
        "learning_rate": 0.0005,
        "perplexity": 14.740207109794904
      },
      {
        "step": 19400,
        "timestamp": 1749094471.8784742,
        "phase": "train",
        "loss": 2.713495969772339,
        "learning_rate": 0.0005,
        "perplexity": 15.08190935732997
      },
      {
        "step": 19450,
        "timestamp": 1749094506.1010945,
        "phase": "train",
        "loss": 2.7148020267486572,
        "learning_rate": 0.0005,
        "perplexity": 15.101620059110244
      },
      {
        "step": 19500,
        "timestamp": 1749094538.1503963,
        "phase": "train",
        "loss": 2.655348539352417,
        "learning_rate": 0.0005,
        "perplexity": 14.229944890883546
      },
      {
        "step": 19550,
        "timestamp": 1749094570.2515624,
        "phase": "train",
        "loss": 2.7200939655303955,
        "learning_rate": 0.0005,
        "perplexity": 15.181748739004803
      },
      {
        "step": 19600,
        "timestamp": 1749094602.3458173,
        "phase": "train",
        "loss": 2.6469104290008545,
        "learning_rate": 0.0005,
        "perplexity": 14.110376221866426
      },
      {
        "step": 19650,
        "timestamp": 1749094634.0170178,
        "phase": "train",
        "loss": 2.588935375213623,
        "learning_rate": 0.0005,
        "perplexity": 13.315587949424629
      },
      {
        "step": 19700,
        "timestamp": 1749094666.0035157,
        "phase": "train",
        "loss": 2.700415849685669,
        "learning_rate": 0.0005,
        "perplexity": 14.885920743394975
      },
      {
        "step": 19750,
        "timestamp": 1749094698.143761,
        "phase": "train",
        "loss": 2.7135260105133057,
        "learning_rate": 0.0005,
        "perplexity": 15.082362435867632
      },
      {
        "step": 19800,
        "timestamp": 1749094730.3873804,
        "phase": "train",
        "loss": 2.7842941284179688,
        "learning_rate": 0.0005,
        "perplexity": 16.188386920557566
      },
      {
        "step": 19850,
        "timestamp": 1749094763.1246557,
        "phase": "train",
        "loss": 2.748199939727783,
        "learning_rate": 0.0005,
        "perplexity": 15.614499531541957
      },
      {
        "step": 19900,
        "timestamp": 1749094795.172746,
        "phase": "train",
        "loss": 2.7594237327575684,
        "learning_rate": 0.0005,
        "perplexity": 15.790740639266897
      },
      {
        "step": 19950,
        "timestamp": 1749094827.2607245,
        "phase": "train",
        "loss": 2.6987175941467285,
        "learning_rate": 0.0005,
        "perplexity": 14.86066209992651
      },
      {
        "step": 20000,
        "timestamp": 1749094859.2769382,
        "phase": "train",
        "loss": 2.7537314891815186,
        "learning_rate": 0.0005,
        "perplexity": 15.70111123551232
      },
      {
        "step": 20050,
        "timestamp": 1749094891.1680684,
        "phase": "train",
        "loss": 2.7143545150756836,
        "learning_rate": 0.0005,
        "perplexity": 15.094863419803222
      },
      {
        "step": 20100,
        "timestamp": 1749094923.1026363,
        "phase": "train",
        "loss": 2.7629027366638184,
        "learning_rate": 0.0005,
        "perplexity": 15.845772359912822
      },
      {
        "step": 20150,
        "timestamp": 1749094955.1057281,
        "phase": "train",
        "loss": 2.6727187633514404,
        "learning_rate": 0.0005,
        "perplexity": 14.479281468011461
      },
      {
        "step": 20200,
        "timestamp": 1749094987.2600415,
        "phase": "train",
        "loss": 2.7246241569519043,
        "learning_rate": 0.0005,
        "perplexity": 15.250680987156128
      },
      {
        "step": 20250,
        "timestamp": 1749095019.3254218,
        "phase": "train",
        "loss": 2.6874501705169678,
        "learning_rate": 0.0005,
        "perplexity": 14.694160508124293
      },
      {
        "step": 20300,
        "timestamp": 1749095050.5931125,
        "phase": "train",
        "loss": 2.646064281463623,
        "learning_rate": 0.0005,
        "perplexity": 14.098441811624854
      },
      {
        "step": 20350,
        "timestamp": 1749095082.2861698,
        "phase": "train",
        "loss": 2.734482765197754,
        "learning_rate": 0.0005,
        "perplexity": 15.40177504127374
      },
      {
        "step": 20400,
        "timestamp": 1749095114.4112542,
        "phase": "train",
        "loss": 2.747964859008789,
        "learning_rate": 0.0005,
        "perplexity": 15.610829295183152
      },
      {
        "step": 20450,
        "timestamp": 1749095146.2422988,
        "phase": "train",
        "loss": 2.6980550289154053,
        "learning_rate": 0.0005,
        "perplexity": 14.85081920304536
      },
      {
        "step": 20500,
        "timestamp": 1749095178.1495616,
        "phase": "train",
        "loss": 2.594712257385254,
        "learning_rate": 0.0005,
        "perplexity": 13.392733146869276
      },
      {
        "step": 20550,
        "timestamp": 1749095209.6146832,
        "phase": "train",
        "loss": 2.6325502395629883,
        "learning_rate": 0.0005,
        "perplexity": 13.909196492968494
      },
      {
        "step": 20600,
        "timestamp": 1749095241.675022,
        "phase": "train",
        "loss": 2.728973388671875,
        "learning_rate": 0.0005,
        "perplexity": 15.317154181536523
      },
      {
        "step": 20650,
        "timestamp": 1749095273.6358354,
        "phase": "train",
        "loss": 2.693345785140991,
        "learning_rate": 0.0005,
        "perplexity": 14.781047490114602
      },
      {
        "step": 20700,
        "timestamp": 1749095305.5614018,
        "phase": "train",
        "loss": 2.788301944732666,
        "learning_rate": 0.0005,
        "perplexity": 16.253397189354935
      },
      {
        "step": 20750,
        "timestamp": 1749095337.3974,
        "phase": "train",
        "loss": 2.607024908065796,
        "learning_rate": 0.0005,
        "perplexity": 13.558652549458273
      },
      {
        "step": 20800,
        "timestamp": 1749095369.496822,
        "phase": "train",
        "loss": 2.714418411254883,
        "learning_rate": 0.0005,
        "perplexity": 15.095827954716
      },
      {
        "step": 20850,
        "timestamp": 1749095401.5480356,
        "phase": "train",
        "loss": 2.7676451206207275,
        "learning_rate": 0.0005,
        "perplexity": 15.921097565888264
      },
      {
        "step": 20900,
        "timestamp": 1749095433.8170803,
        "phase": "train",
        "loss": 2.6978037357330322,
        "learning_rate": 0.0005,
        "perplexity": 14.847087762289426
      },
      {
        "step": 20950,
        "timestamp": 1749095466.303037,
        "phase": "train",
        "loss": 2.7280964851379395,
        "learning_rate": 0.0005,
        "perplexity": 15.303728402321841
      },
      {
        "step": 21000,
        "timestamp": 1749095498.9867132,
        "phase": "train",
        "loss": 2.745905876159668,
        "learning_rate": 0.0005,
        "perplexity": 15.578719933060496
      },
      {
        "step": 21050,
        "timestamp": 1749095531.5726364,
        "phase": "train",
        "loss": 2.7705774307250977,
        "learning_rate": 0.0005,
        "perplexity": 15.967851676427719
      },
      {
        "step": 21100,
        "timestamp": 1749095563.551192,
        "phase": "train",
        "loss": 2.651732921600342,
        "learning_rate": 0.0005,
        "perplexity": 14.178587749369289
      },
      {
        "step": 21150,
        "timestamp": 1749095596.1457665,
        "phase": "train",
        "loss": 2.6501071453094482,
        "learning_rate": 0.0005,
        "perplexity": 14.155555265474318
      },
      {
        "step": 21200,
        "timestamp": 1749095628.2338014,
        "phase": "train",
        "loss": 2.7176437377929688,
        "learning_rate": 0.0005,
        "perplexity": 15.144595532638034
      },
      {
        "step": 21250,
        "timestamp": 1749095660.264628,
        "phase": "train",
        "loss": 2.628986120223999,
        "learning_rate": 0.0005,
        "perplexity": 13.859710695786085
      },
      {
        "step": 21300,
        "timestamp": 1749095692.2009673,
        "phase": "train",
        "loss": 2.6962063312530518,
        "learning_rate": 0.0005,
        "perplexity": 14.823389890365828
      },
      {
        "step": 21350,
        "timestamp": 1749095723.657312,
        "phase": "train",
        "loss": 2.7408039569854736,
        "learning_rate": 0.0005,
        "perplexity": 15.499440972515492
      },
      {
        "step": 21400,
        "timestamp": 1749095755.2959657,
        "phase": "train",
        "loss": 2.7504653930664062,
        "learning_rate": 0.0005,
        "perplexity": 15.649913550894588
      },
      {
        "step": 21450,
        "timestamp": 1749095787.2657287,
        "phase": "train",
        "loss": 2.655932903289795,
        "learning_rate": 0.0005,
        "perplexity": 14.238262787611395
      },
      {
        "step": 21500,
        "timestamp": 1749095819.1621785,
        "phase": "train",
        "loss": 2.7180631160736084,
        "learning_rate": 0.0005,
        "perplexity": 15.150948179061354
      },
      {
        "step": 21550,
        "timestamp": 1749095850.8781424,
        "phase": "train",
        "loss": 2.703838348388672,
        "learning_rate": 0.0005,
        "perplexity": 14.936955070475507
      },
      {
        "step": 21600,
        "timestamp": 1749095883.5112271,
        "phase": "train",
        "loss": 2.6073951721191406,
        "learning_rate": 0.0005,
        "perplexity": 13.563673760638766
      },
      {
        "step": 21650,
        "timestamp": 1749095915.7476563,
        "phase": "train",
        "loss": 2.7783594131469727,
        "learning_rate": 0.0005,
        "perplexity": 16.09259797469438
      },
      {
        "step": 21700,
        "timestamp": 1749095948.2528927,
        "phase": "train",
        "loss": 2.750808000564575,
        "learning_rate": 0.0005,
        "perplexity": 15.65527624722035
      },
      {
        "step": 21750,
        "timestamp": 1749095980.6238332,
        "phase": "train",
        "loss": 2.6959972381591797,
        "learning_rate": 0.0005,
        "perplexity": 14.82029074592812
      },
      {
        "step": 21800,
        "timestamp": 1749096012.6230364,
        "phase": "train",
        "loss": 2.7145280838012695,
        "learning_rate": 0.0005,
        "perplexity": 15.097483643397748
      },
      {
        "step": 21850,
        "timestamp": 1749096044.4632282,
        "phase": "train",
        "loss": 2.7371888160705566,
        "learning_rate": 0.0005,
        "perplexity": 15.44350947034357
      },
      {
        "step": 21900,
        "timestamp": 1749096076.8694122,
        "phase": "train",
        "loss": 2.6984567642211914,
        "learning_rate": 0.0005,
        "perplexity": 14.856786499995746
      },
      {
        "step": 21950,
        "timestamp": 1749096109.2096856,
        "phase": "train",
        "loss": 2.685830593109131,
        "learning_rate": 0.0005,
        "perplexity": 14.670381438957575
      },
      {
        "step": 22000,
        "timestamp": 1749096141.0119119,
        "phase": "train",
        "loss": 2.7114202976226807,
        "learning_rate": 0.0005,
        "perplexity": 15.050636725207019
      },
      {
        "step": 22050,
        "timestamp": 1749096172.932776,
        "phase": "train",
        "loss": 2.6794075965881348,
        "learning_rate": 0.0005,
        "perplexity": 14.576455595604106
      },
      {
        "step": 22100,
        "timestamp": 1749096205.1908164,
        "phase": "train",
        "loss": 2.683488368988037,
        "learning_rate": 0.0005,
        "perplexity": 14.636060327243968
      },
      {
        "step": 22150,
        "timestamp": 1749096237.3257751,
        "phase": "train",
        "loss": 2.6915197372436523,
        "learning_rate": 0.0005,
        "perplexity": 14.75408121777041
      },
      {
        "step": 22200,
        "timestamp": 1749096269.1008906,
        "phase": "train",
        "loss": 2.6959028244018555,
        "learning_rate": 0.0005,
        "perplexity": 14.818891572645802
      },
      {
        "step": 22250,
        "timestamp": 1749096301.2342978,
        "phase": "train",
        "loss": 2.731388568878174,
        "learning_rate": 0.0005,
        "perplexity": 15.354192578329961
      },
      {
        "step": 22300,
        "timestamp": 1749096332.8392313,
        "phase": "train",
        "loss": 2.7241415977478027,
        "learning_rate": 0.0005,
        "perplexity": 15.243323406053976
      },
      {
        "step": 22350,
        "timestamp": 1749096364.8641095,
        "phase": "train",
        "loss": 2.8139235973358154,
        "learning_rate": 0.0005,
        "perplexity": 16.675216868691827
      },
      {
        "step": 22400,
        "timestamp": 1749096396.9064653,
        "phase": "train",
        "loss": 2.660080909729004,
        "learning_rate": 0.0005,
        "perplexity": 14.2974458543501
      },
      {
        "step": 22450,
        "timestamp": 1749096428.2949805,
        "phase": "train",
        "loss": 2.6933817863464355,
        "learning_rate": 0.0005,
        "perplexity": 14.78157963522085
      },
      {
        "step": 22500,
        "timestamp": 1749096460.4328668,
        "phase": "train",
        "loss": 2.6642723083496094,
        "learning_rate": 0.0005,
        "perplexity": 14.357497912323382
      },
      {
        "step": 22550,
        "timestamp": 1749096492.0095823,
        "phase": "train",
        "loss": 2.627979278564453,
        "learning_rate": 0.0005,
        "perplexity": 13.845763184314363
      },
      {
        "step": 22600,
        "timestamp": 1749096524.2070322,
        "phase": "train",
        "loss": 2.6558542251586914,
        "learning_rate": 0.0005,
        "perplexity": 14.237142591773141
      },
      {
        "step": 22650,
        "timestamp": 1749096555.7213814,
        "phase": "train",
        "loss": 2.644310474395752,
        "learning_rate": 0.0005,
        "perplexity": 14.073737534329918
      },
      {
        "step": 22700,
        "timestamp": 1749096587.3716187,
        "phase": "train",
        "loss": 2.5779309272766113,
        "learning_rate": 0.0005,
        "perplexity": 13.169860550475404
      },
      {
        "step": 22750,
        "timestamp": 1749096619.1990101,
        "phase": "train",
        "loss": 2.6788477897644043,
        "learning_rate": 0.0005,
        "perplexity": 14.568297879881387
      },
      {
        "step": 22800,
        "timestamp": 1749096651.4823875,
        "phase": "train",
        "loss": 2.6376190185546875,
        "learning_rate": 0.0005,
        "perplexity": 13.97987811938264
      },
      {
        "step": 22850,
        "timestamp": 1749096683.1045105,
        "phase": "train",
        "loss": 2.6468091011047363,
        "learning_rate": 0.0005,
        "perplexity": 14.108946519566015
      },
      {
        "step": 22900,
        "timestamp": 1749096714.8568637,
        "phase": "train",
        "loss": 2.665361166000366,
        "learning_rate": 0.0005,
        "perplexity": 14.373139698064556
      },
      {
        "step": 22950,
        "timestamp": 1749096746.600855,
        "phase": "train",
        "loss": 2.678999662399292,
        "learning_rate": 0.0005,
        "perplexity": 14.5705105736858
      },
      {
        "step": 23000,
        "timestamp": 1749096778.9138622,
        "phase": "train",
        "loss": 2.6813526153564453,
        "learning_rate": 0.0005,
        "perplexity": 14.604834665277838
      },
      {
        "step": 23050,
        "timestamp": 1749096811.3822923,
        "phase": "train",
        "loss": 2.631978750228882,
        "learning_rate": 0.0005,
        "perplexity": 13.901249806466343
      },
      {
        "step": 23100,
        "timestamp": 1749096843.0564685,
        "phase": "train",
        "loss": 2.6914401054382324,
        "learning_rate": 0.0005,
        "perplexity": 14.752906370423956
      },
      {
        "step": 23150,
        "timestamp": 1749096875.1884441,
        "phase": "train",
        "loss": 2.579509735107422,
        "learning_rate": 0.0005,
        "perplexity": 13.190669651906868
      },
      {
        "step": 23200,
        "timestamp": 1749096906.9588156,
        "phase": "train",
        "loss": 2.6603548526763916,
        "learning_rate": 0.0005,
        "perplexity": 14.30136307533059
      },
      {
        "step": 23250,
        "timestamp": 1749096938.7702827,
        "phase": "train",
        "loss": 2.62916898727417,
        "learning_rate": 0.0005,
        "perplexity": 13.862245411948214
      },
      {
        "step": 23300,
        "timestamp": 1749096970.4778225,
        "phase": "train",
        "loss": 2.652357578277588,
        "learning_rate": 0.0005,
        "perplexity": 14.18744726567078
      },
      {
        "step": 23350,
        "timestamp": 1749097002.3352134,
        "phase": "train",
        "loss": 2.7230231761932373,
        "learning_rate": 0.0005,
        "perplexity": 15.226284474723586
      },
      {
        "step": 23400,
        "timestamp": 1749097033.9652174,
        "phase": "train",
        "loss": 2.538881301879883,
        "learning_rate": 0.0005,
        "perplexity": 12.665494178009345
      },
      {
        "step": 23450,
        "timestamp": 1749097065.9341493,
        "phase": "train",
        "loss": 2.705268383026123,
        "learning_rate": 0.0005,
        "perplexity": 14.958330713916835
      },
      {
        "step": 23500,
        "timestamp": 1749097097.9158816,
        "phase": "train",
        "loss": 2.7247118949890137,
        "learning_rate": 0.0005,
        "perplexity": 15.25201911067183
      },
      {
        "step": 23550,
        "timestamp": 1749097129.8832514,
        "phase": "train",
        "loss": 2.6895298957824707,
        "learning_rate": 0.0005,
        "perplexity": 14.724752125041318
      },
      {
        "step": 23600,
        "timestamp": 1749097161.8923707,
        "phase": "train",
        "loss": 2.676797866821289,
        "learning_rate": 0.0005,
        "perplexity": 14.538464580245062
      },
      {
        "step": 23650,
        "timestamp": 1749097194.101593,
        "phase": "train",
        "loss": 2.721992015838623,
        "learning_rate": 0.0005,
        "perplexity": 15.210591826034255
      },
      {
        "step": 23700,
        "timestamp": 1749097226.2283492,
        "phase": "train",
        "loss": 2.690051555633545,
        "learning_rate": 0.0005,
        "perplexity": 14.732435440906439
      },
      {
        "step": 23750,
        "timestamp": 1749097258.0191083,
        "phase": "train",
        "loss": 2.7891461849212646,
        "learning_rate": 0.0005,
        "perplexity": 16.267124754329117
      },
      {
        "step": 23800,
        "timestamp": 1749097289.8315816,
        "phase": "train",
        "loss": 2.697382688522339,
        "learning_rate": 0.0005,
        "perplexity": 14.840837753266962
      },
      {
        "step": 23850,
        "timestamp": 1749097322.0274649,
        "phase": "train",
        "loss": 2.7433531284332275,
        "learning_rate": 0.0005,
        "perplexity": 15.539002107533431
      },
      {
        "step": 23900,
        "timestamp": 1749097353.8479118,
        "phase": "train",
        "loss": 2.751389265060425,
        "learning_rate": 0.0005,
        "perplexity": 15.664378748699756
      },
      {
        "step": 23950,
        "timestamp": 1749097385.8916671,
        "phase": "train",
        "loss": 2.709271192550659,
        "learning_rate": 0.0005,
        "perplexity": 15.018326057429702
      },
      {
        "step": 24000,
        "timestamp": 1749097417.5317898,
        "phase": "train",
        "loss": 2.610060214996338,
        "learning_rate": 0.0005,
        "perplexity": 13.599869743282692
      },
      {
        "step": 24050,
        "timestamp": 1749097450.3723533,
        "phase": "train",
        "loss": 2.6425907611846924,
        "learning_rate": 0.0005,
        "perplexity": 14.049555540969552
      },
      {
        "step": 24100,
        "timestamp": 1749097482.3237753,
        "phase": "train",
        "loss": 2.732976198196411,
        "learning_rate": 0.0005,
        "perplexity": 15.378588705504217
      },
      {
        "step": 24150,
        "timestamp": 1749097514.4873843,
        "phase": "train",
        "loss": 2.6660048961639404,
        "learning_rate": 0.0005,
        "perplexity": 14.382395100304654
      },
      {
        "step": 24200,
        "timestamp": 1749097546.3229847,
        "phase": "train",
        "loss": 2.644505023956299,
        "learning_rate": 0.0005,
        "perplexity": 14.076475840141933
      },
      {
        "step": 24250,
        "timestamp": 1749097579.0802698,
        "phase": "train",
        "loss": 2.7693538665771484,
        "learning_rate": 0.0005,
        "perplexity": 15.94832593353217
      },
      {
        "step": 24300,
        "timestamp": 1749097610.9758286,
        "phase": "train",
        "loss": 2.7275984287261963,
        "learning_rate": 0.0005,
        "perplexity": 15.296108180075286
      },
      {
        "step": 24350,
        "timestamp": 1749097642.960828,
        "phase": "train",
        "loss": 2.690319061279297,
        "learning_rate": 0.0005,
        "perplexity": 14.73637697773073
      },
      {
        "step": 24400,
        "timestamp": 1749097674.972468,
        "phase": "train",
        "loss": 2.666713237762451,
        "learning_rate": 0.0005,
        "perplexity": 14.392586358060202
      },
      {
        "step": 24450,
        "timestamp": 1749097706.9317956,
        "phase": "train",
        "loss": 2.687502861022949,
        "learning_rate": 0.0005,
        "perplexity": 14.694934771274417
      },
      {
        "step": 24500,
        "timestamp": 1749097738.678353,
        "phase": "train",
        "loss": 2.6215462684631348,
        "learning_rate": 0.0005,
        "perplexity": 13.756979130430517
      },
      {
        "step": 24550,
        "timestamp": 1749097770.6577716,
        "phase": "train",
        "loss": 2.6006274223327637,
        "learning_rate": 0.0005,
        "perplexity": 13.472188135541021
      },
      {
        "step": 24600,
        "timestamp": 1749097802.1317546,
        "phase": "train",
        "loss": 2.7326998710632324,
        "learning_rate": 0.0005,
        "perplexity": 15.374339771249838
      },
      {
        "step": 24650,
        "timestamp": 1749097834.566727,
        "phase": "train",
        "loss": 2.672315835952759,
        "learning_rate": 0.0005,
        "perplexity": 14.47344854399614
      },
      {
        "step": 24700,
        "timestamp": 1749097866.1086853,
        "phase": "train",
        "loss": 2.6349282264709473,
        "learning_rate": 0.0005,
        "perplexity": 13.942311738333903
      },
      {
        "step": 24750,
        "timestamp": 1749097897.9447908,
        "phase": "train",
        "loss": 2.720559597015381,
        "learning_rate": 0.0005,
        "perplexity": 15.188819485268109
      },
      {
        "step": 24800,
        "timestamp": 1749097930.2561638,
        "phase": "train",
        "loss": 2.7553234100341797,
        "learning_rate": 0.0005,
        "perplexity": 15.726126067431624
      },
      {
        "step": 24850,
        "timestamp": 1749097961.6510327,
        "phase": "train",
        "loss": 2.6426267623901367,
        "learning_rate": 0.0005,
        "perplexity": 14.050061351009814
      },
      {
        "step": 24900,
        "timestamp": 1749097993.944686,
        "phase": "train",
        "loss": 2.6965208053588867,
        "learning_rate": 0.0005,
        "perplexity": 14.82805219569577
      },
      {
        "step": 24950,
        "timestamp": 1749098026.0745788,
        "phase": "train",
        "loss": 2.6335010528564453,
        "learning_rate": 0.0005,
        "perplexity": 13.922427831164626
      },
      {
        "step": 25000,
        "timestamp": 1749098058.0010421,
        "phase": "train",
        "loss": 2.6510956287384033,
        "learning_rate": 0.0005,
        "perplexity": 14.169554715254053
      },
      {
        "step": 25050,
        "timestamp": 1749098090.0839524,
        "phase": "train",
        "loss": 2.6337594985961914,
        "learning_rate": 0.0005,
        "perplexity": 13.926026488333289
      },
      {
        "step": 25100,
        "timestamp": 1749098122.406704,
        "phase": "train",
        "loss": 2.732975959777832,
        "learning_rate": 0.0005,
        "perplexity": 15.378585038963386
      },
      {
        "step": 25150,
        "timestamp": 1749098154.6027927,
        "phase": "train",
        "loss": 2.5277905464172363,
        "learning_rate": 0.0005,
        "perplexity": 12.52580036613621
      },
      {
        "step": 25200,
        "timestamp": 1749098186.9933076,
        "phase": "train",
        "loss": 2.6193389892578125,
        "learning_rate": 0.0005,
        "perplexity": 13.726647124385964
      },
      {
        "step": 25250,
        "timestamp": 1749098218.9480245,
        "phase": "train",
        "loss": 2.6164960861206055,
        "learning_rate": 0.0005,
        "perplexity": 13.687679013740055
      },
      {
        "step": 25300,
        "timestamp": 1749098250.9238605,
        "phase": "train",
        "loss": 2.7148618698120117,
        "learning_rate": 0.0005,
        "perplexity": 15.102523813357639
      },
      {
        "step": 25350,
        "timestamp": 1749098282.9755692,
        "phase": "train",
        "loss": 2.671135425567627,
        "learning_rate": 0.0005,
        "perplexity": 14.456374014484707
      },
      {
        "step": 25400,
        "timestamp": 1749098315.3674846,
        "phase": "train",
        "loss": 2.6859915256500244,
        "learning_rate": 0.0005,
        "perplexity": 14.672742570704795
      },
      {
        "step": 25450,
        "timestamp": 1749098347.3754148,
        "phase": "train",
        "loss": 2.7032783031463623,
        "learning_rate": 0.0005,
        "perplexity": 14.928592041909464
      },
      {
        "step": 25500,
        "timestamp": 1749098379.6283257,
        "phase": "train",
        "loss": 2.689178466796875,
        "learning_rate": 0.0005,
        "perplexity": 14.719578329502973
      },
      {
        "step": 25550,
        "timestamp": 1749098411.413174,
        "phase": "train",
        "loss": 2.7019782066345215,
        "learning_rate": 0.0005,
        "perplexity": 14.909196042536657
      },
      {
        "step": 25600,
        "timestamp": 1749098443.5299706,
        "phase": "train",
        "loss": 2.6294331550598145,
        "learning_rate": 0.0005,
        "perplexity": 13.865907854351102
      },
      {
        "step": 25650,
        "timestamp": 1749098476.2439451,
        "phase": "train",
        "loss": 2.6709251403808594,
        "learning_rate": 0.0005,
        "perplexity": 14.453334372782102
      },
      {
        "step": 25700,
        "timestamp": 1749098508.2307281,
        "phase": "train",
        "loss": 2.6620404720306396,
        "learning_rate": 0.0005,
        "perplexity": 14.325490058464668
      },
      {
        "step": 25750,
        "timestamp": 1749098540.431533,
        "phase": "train",
        "loss": 2.677236318588257,
        "learning_rate": 0.0005,
        "perplexity": 14.544840393370393
      },
      {
        "step": 25800,
        "timestamp": 1749098572.2718613,
        "phase": "train",
        "loss": 2.640474557876587,
        "learning_rate": 0.0005,
        "perplexity": 14.019855262054522
      },
      {
        "step": 25850,
        "timestamp": 1749098603.91773,
        "phase": "train",
        "loss": 2.64052414894104,
        "learning_rate": 0.0005,
        "perplexity": 14.020550538840062
      },
      {
        "step": 25900,
        "timestamp": 1749098636.4052467,
        "phase": "train",
        "loss": 2.701331615447998,
        "learning_rate": 0.0005,
        "perplexity": 14.899559003725336
      },
      {
        "step": 25950,
        "timestamp": 1749098668.1877623,
        "phase": "train",
        "loss": 2.6608214378356934,
        "learning_rate": 0.0005,
        "perplexity": 14.308037436057072
      },
      {
        "step": 26000,
        "timestamp": 1749098700.5998437,
        "phase": "train",
        "loss": 2.6195168495178223,
        "learning_rate": 0.0005,
        "perplexity": 13.729088766541688
      },
      {
        "step": 26050,
        "timestamp": 1749098732.431291,
        "phase": "train",
        "loss": 2.595327854156494,
        "learning_rate": 0.0005,
        "perplexity": 13.40098020832433
      },
      {
        "step": 26100,
        "timestamp": 1749098764.3755405,
        "phase": "train",
        "loss": 2.7619359493255615,
        "learning_rate": 0.0005,
        "perplexity": 15.830460270789956
      },
      {
        "step": 26150,
        "timestamp": 1749098796.276097,
        "phase": "train",
        "loss": 2.669102191925049,
        "learning_rate": 0.0005,
        "perplexity": 14.427010689854274
      },
      {
        "step": 26200,
        "timestamp": 1749098829.011262,
        "phase": "train",
        "loss": 2.6965465545654297,
        "learning_rate": 0.0005,
        "perplexity": 14.828434011190089
      },
      {
        "step": 26250,
        "timestamp": 1749098861.4900753,
        "phase": "train",
        "loss": 2.654632568359375,
        "learning_rate": 0.0005,
        "perplexity": 14.219760309476584
      },
      {
        "step": 26300,
        "timestamp": 1749098893.7020814,
        "phase": "train",
        "loss": 2.6688263416290283,
        "learning_rate": 0.0005,
        "perplexity": 14.423031543534364
      },
      {
        "step": 26350,
        "timestamp": 1749098926.0895903,
        "phase": "train",
        "loss": 2.7204747200012207,
        "learning_rate": 0.0005,
        "perplexity": 15.18753035833098
      },
      {
        "step": 26400,
        "timestamp": 1749098958.5564373,
        "phase": "train",
        "loss": 2.7736103534698486,
        "learning_rate": 0.0005,
        "perplexity": 16.016354452368255
      },
      {
        "step": 26450,
        "timestamp": 1749098990.6870317,
        "phase": "train",
        "loss": 2.604074716567993,
        "learning_rate": 0.0005,
        "perplexity": 13.518710874749559
      },
      {
        "step": 26500,
        "timestamp": 1749099022.326813,
        "phase": "train",
        "loss": 2.6821465492248535,
        "learning_rate": 0.0005,
        "perplexity": 14.616434542319418
      },
      {
        "step": 26550,
        "timestamp": 1749099054.42444,
        "phase": "train",
        "loss": 2.6866135597229004,
        "learning_rate": 0.0005,
        "perplexity": 14.681872355750619
      },
      {
        "step": 26600,
        "timestamp": 1749099086.6614447,
        "phase": "train",
        "loss": 2.653444766998291,
        "learning_rate": 0.0005,
        "perplexity": 14.20288008598757
      },
      {
        "step": 26650,
        "timestamp": 1749099118.3823924,
        "phase": "train",
        "loss": 2.705291509628296,
        "learning_rate": 0.0005,
        "perplexity": 14.95867665328061
      },
      {
        "step": 26700,
        "timestamp": 1749099150.271874,
        "phase": "train",
        "loss": 2.6762423515319824,
        "learning_rate": 0.0005,
        "perplexity": 14.530390483737358
      },
      {
        "step": 26750,
        "timestamp": 1749099182.2775083,
        "phase": "train",
        "loss": 2.6408345699310303,
        "learning_rate": 0.0005,
        "perplexity": 14.024903487606915
      },
      {
        "step": 26800,
        "timestamp": 1749099213.8257146,
        "phase": "train",
        "loss": 2.6326022148132324,
        "learning_rate": 0.0005,
        "perplexity": 13.909919445724572
      },
      {
        "step": 26850,
        "timestamp": 1749099246.03382,
        "phase": "train",
        "loss": 2.68131422996521,
        "learning_rate": 0.0005,
        "perplexity": 14.604274063744809
      },
      {
        "step": 26900,
        "timestamp": 1749099278.3854933,
        "phase": "train",
        "loss": 2.7113559246063232,
        "learning_rate": 0.0005,
        "perplexity": 15.049667901506304
      },
      {
        "step": 26950,
        "timestamp": 1749099310.0528011,
        "phase": "train",
        "loss": 2.617819309234619,
        "learning_rate": 0.0005,
        "perplexity": 13.705802855286812
      },
      {
        "step": 27000,
        "timestamp": 1749099342.0546086,
        "phase": "train",
        "loss": 2.752800941467285,
        "learning_rate": 0.0005,
        "perplexity": 15.686507398178726
      },
      {
        "step": 27050,
        "timestamp": 1749099374.2401767,
        "phase": "train",
        "loss": 2.639446258544922,
        "learning_rate": 0.0005,
        "perplexity": 14.00544606401256
      },
      {
        "step": 27100,
        "timestamp": 1749099406.501738,
        "phase": "train",
        "loss": 2.6568069458007812,
        "learning_rate": 0.0005,
        "perplexity": 14.250713074817892
      },
      {
        "step": 27150,
        "timestamp": 1749099438.2179043,
        "phase": "train",
        "loss": 2.6838817596435547,
        "learning_rate": 0.0005,
        "perplexity": 14.641819149269416
      },
      {
        "step": 27200,
        "timestamp": 1749099470.7271569,
        "phase": "train",
        "loss": 2.6873855590820312,
        "learning_rate": 0.0005,
        "perplexity": 14.693211127999412
      },
      {
        "step": 27250,
        "timestamp": 1749099502.5508957,
        "phase": "train",
        "loss": 2.670304536819458,
        "learning_rate": 0.0005,
        "perplexity": 14.444367364762588
      },
      {
        "step": 27300,
        "timestamp": 1749099534.7995102,
        "phase": "train",
        "loss": 2.6575076580047607,
        "learning_rate": 0.0005,
        "perplexity": 14.260702222735032
      },
      {
        "step": 27350,
        "timestamp": 1749099566.9152048,
        "phase": "train",
        "loss": 2.635735273361206,
        "learning_rate": 0.0005,
        "perplexity": 13.953568379372939
      },
      {
        "step": 27400,
        "timestamp": 1749099598.4300315,
        "phase": "train",
        "loss": 2.741853713989258,
        "learning_rate": 0.0005,
        "perplexity": 15.515720162332935
      },
      {
        "step": 27450,
        "timestamp": 1749099631.0486672,
        "phase": "train",
        "loss": 2.637434720993042,
        "learning_rate": 0.0005,
        "perplexity": 13.977301899335965
      },
      {
        "step": 27500,
        "timestamp": 1749099663.1142416,
        "phase": "train",
        "loss": 2.6795597076416016,
        "learning_rate": 0.0005,
        "perplexity": 14.578673004262473
      },
      {
        "step": 27550,
        "timestamp": 1749099695.425073,
        "phase": "train",
        "loss": 2.6924657821655273,
        "learning_rate": 0.0005,
        "perplexity": 14.768045845924659
      },
      {
        "step": 27600,
        "timestamp": 1749099727.4580777,
        "phase": "train",
        "loss": 2.6346724033355713,
        "learning_rate": 0.0005,
        "perplexity": 13.938745428622328
      },
      {
        "step": 27650,
        "timestamp": 1749099759.3095238,
        "phase": "train",
        "loss": 2.6225554943084717,
        "learning_rate": 0.0005,
        "perplexity": 13.770870037674978
      },
      {
        "step": 27700,
        "timestamp": 1749099791.3386238,
        "phase": "train",
        "loss": 2.7222986221313477,
        "learning_rate": 0.0005,
        "perplexity": 15.215256204231496
      },
      {
        "step": 27750,
        "timestamp": 1749099823.8806145,
        "phase": "train",
        "loss": 2.662266492843628,
        "learning_rate": 0.0005,
        "perplexity": 14.32872828331346
      },
      {
        "step": 27800,
        "timestamp": 1749099855.7392771,
        "phase": "train",
        "loss": 2.687413215637207,
        "learning_rate": 0.0005,
        "perplexity": 14.693617497223043
      },
      {
        "step": 27850,
        "timestamp": 1749099887.259031,
        "phase": "train",
        "loss": 2.6636767387390137,
        "learning_rate": 0.0005,
        "perplexity": 14.34894956870213
      },
      {
        "step": 27900,
        "timestamp": 1749099919.204572,
        "phase": "train",
        "loss": 2.689852714538574,
        "learning_rate": 0.0005,
        "perplexity": 14.729506318536384
      },
      {
        "step": 27950,
        "timestamp": 1749099951.2945058,
        "phase": "train",
        "loss": 2.6733193397521973,
        "learning_rate": 0.0005,
        "perplexity": 14.487979994564492
      },
      {
        "step": 28000,
        "timestamp": 1749099983.588468,
        "phase": "train",
        "loss": 2.7757747173309326,
        "learning_rate": 0.0005,
        "perplexity": 16.051057212283986
      },
      {
        "step": 28050,
        "timestamp": 1749100015.5895574,
        "phase": "train",
        "loss": 2.6933376789093018,
        "learning_rate": 0.0005,
        "perplexity": 14.78092767200467
      },
      {
        "step": 28100,
        "timestamp": 1749100047.6621861,
        "phase": "train",
        "loss": 2.6929819583892822,
        "learning_rate": 0.0005,
        "perplexity": 14.775670727783714
      },
      {
        "step": 28150,
        "timestamp": 1749100079.4079168,
        "phase": "train",
        "loss": 2.5615689754486084,
        "learning_rate": 0.0005,
        "perplexity": 12.956129225596149
      },
      {
        "step": 28200,
        "timestamp": 1749100111.0856168,
        "phase": "train",
        "loss": 2.7352283000946045,
        "learning_rate": 0.0005,
        "perplexity": 15.413261883429241
      },
      {
        "step": 28250,
        "timestamp": 1749100143.2541602,
        "phase": "train",
        "loss": 2.6919054985046387,
        "learning_rate": 0.0005,
        "perplexity": 14.759773868677174
      },
      {
        "step": 28300,
        "timestamp": 1749100174.8770149,
        "phase": "train",
        "loss": 2.742448568344116,
        "learning_rate": 0.0005,
        "perplexity": 15.524952501716667
      },
      {
        "step": 28350,
        "timestamp": 1749100206.849533,
        "phase": "train",
        "loss": 2.7287840843200684,
        "learning_rate": 0.0005,
        "perplexity": 15.314254852029169
      },
      {
        "step": 28400,
        "timestamp": 1749100238.8678823,
        "phase": "train",
        "loss": 2.625692367553711,
        "learning_rate": 0.0005,
        "perplexity": 13.814135334858976
      },
      {
        "step": 28450,
        "timestamp": 1749100270.8065097,
        "phase": "train",
        "loss": 2.677396297454834,
        "learning_rate": 0.0005,
        "perplexity": 14.547167446585775
      },
      {
        "step": 28500,
        "timestamp": 1749100301.8720176,
        "phase": "train",
        "loss": 2.761176586151123,
        "learning_rate": 0.0005,
        "perplexity": 15.81844376524923
      },
      {
        "step": 28550,
        "timestamp": 1749100334.2264009,
        "phase": "train",
        "loss": 2.6963305473327637,
        "learning_rate": 0.0005,
        "perplexity": 14.82523130811053
      },
      {
        "step": 28600,
        "timestamp": 1749100366.6680799,
        "phase": "train",
        "loss": 2.641322135925293,
        "learning_rate": 0.0005,
        "perplexity": 14.031743220895473
      },
      {
        "step": 28650,
        "timestamp": 1749100398.463412,
        "phase": "train",
        "loss": 2.7326974868774414,
        "learning_rate": 0.0005,
        "perplexity": 15.374303116011106
      },
      {
        "step": 28700,
        "timestamp": 1749100430.8524415,
        "phase": "train",
        "loss": 2.671334981918335,
        "learning_rate": 0.0005,
        "perplexity": 14.459259163592847
      },
      {
        "step": 28750,
        "timestamp": 1749100463.1440036,
        "phase": "train",
        "loss": 2.6964728832244873,
        "learning_rate": 0.0005,
        "perplexity": 14.827341620811836
      },
      {
        "step": 28800,
        "timestamp": 1749100494.7652574,
        "phase": "train",
        "loss": 2.650729179382324,
        "learning_rate": 0.0005,
        "perplexity": 14.164363242316773
      },
      {
        "step": 28850,
        "timestamp": 1749100526.9529192,
        "phase": "train",
        "loss": 2.595247268676758,
        "learning_rate": 0.0005,
        "perplexity": 13.399900327417251
      },
      {
        "step": 28900,
        "timestamp": 1749100559.0025065,
        "phase": "train",
        "loss": 2.63561749458313,
        "learning_rate": 0.0005,
        "perplexity": 13.951925041916452
      },
      {
        "step": 28950,
        "timestamp": 1749100590.6007771,
        "phase": "train",
        "loss": 2.601125717163086,
        "learning_rate": 0.0005,
        "perplexity": 13.478902930076854
      },
      {
        "step": 29000,
        "timestamp": 1749100623.2487805,
        "phase": "train",
        "loss": 2.7669687271118164,
        "learning_rate": 0.0005,
        "perplexity": 15.910332280035126
      },
      {
        "step": 29050,
        "timestamp": 1749100655.0174928,
        "phase": "train",
        "loss": 2.74833345413208,
        "learning_rate": 0.0005,
        "perplexity": 15.616584431324284
      },
      {
        "step": 29100,
        "timestamp": 1749100687.7496438,
        "phase": "train",
        "loss": 2.6672630310058594,
        "learning_rate": 0.0005,
        "perplexity": 14.400501480436057
      },
      {
        "step": 29150,
        "timestamp": 1749100719.7078671,
        "phase": "train",
        "loss": 2.659484386444092,
        "learning_rate": 0.0005,
        "perplexity": 14.28891963827925
      }
    ]
  },
  "events": [
    {
      "timestamp": 1749081995.8165097,
      "event": "training_started",
      "data": {}
    },
    {
      "timestamp": 1749081995.816916,
      "event": "epoch_started",
      "data": {
        "epoch": 1
      }
    },
    {
      "timestamp": 1749088270.9276602,
      "event": "epoch_completed",
      "data": {
        "epoch": 1,
        "duration": 6275.106712579727
      }
    },
    {
      "timestamp": 1749088272.9471793,
      "event": "epoch_started",
      "data": {
        "epoch": 2
      }
    },
    {
      "timestamp": 1749094500.094577,
      "event": "epoch_completed",
      "data": {
        "epoch": 2,
        "duration": 6227.140292167664
      }
    },
    {
      "timestamp": 1749094501.5723102,
      "event": "epoch_started",
      "data": {
        "epoch": 3
      }
    },
    {
      "timestamp": 1749100730.094087,
      "event": "epoch_completed",
      "data": {
        "epoch": 3,
        "duration": 6228.514485120773
      }
    },
    {
      "timestamp": 1749100731.633558,
      "event": "training_completed",
      "data": {
        "total_time": 18735.805684804916,
        "total_steps": 29166
      }
    }
  ]
}