#!/usr/bin/env python3
# test_model_comparison.py
"""
Compare original TFT model with refactored version to ensure equivalent functionality.
"""

import sys
import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path

# Add paths for both implementations
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_imports():
    """Test that both models can be imported."""
    print("üîç Testing imports...")
    
    try:
        # Original model
        from src.models.model_token_factored_alibi import FactoredTransformerModelALiBi
        from src.config.config_alibi import GPTConfigALiBi
        print("  ‚úÖ Original model imported")
        
        # New model
        from models.tft_alibi import TokenFactoredTransformer, TFTConfig
        print("  ‚úÖ New model imported")
        
        return True, (FactoredTransformerModelALiBi, GPTConfigALiBi, TokenFactoredTransformer, TFTConfig)
        
    except Exception as e:
        print(f"  ‚ùå Import failed: {e}")
        return False, None

def create_equivalent_configs():
    """Create equivalent configs for both models."""
    print("\n‚öôÔ∏è Creating equivalent configurations...")
    
    # Common parameters
    common_params = {
        'vocab_size': 1000,
        'n_layer': 2,      # Original uses n_layer
        'n_head': 4,       # Original uses n_head  
        'n_embd': 128,     # Original uses n_embd
        'block_size': 32,
        'dropout': 0.0,    # Disable for deterministic comparison
        'bias': False,
        'use_v': True,     # Test with factorization enabled
        'use_proj': True,
    }
    
    try:
        # Original config
        from src.config.config_alibi import GPTConfigALiBi
        original_config = GPTConfigALiBi(
            vocab_size=common_params['vocab_size'],
            n_layer=common_params['n_layer'],
            n_head=common_params['n_head'],
            n_embd=common_params['n_embd'],
            block_size=common_params['block_size'],
            dropout=common_params['dropout'],
            bias=common_params['bias'],
            use_v=common_params['use_v'],
            use_proj=common_params['use_proj'],
            max_position_embeddings=64,
        )
        
        # New config  
        from models.tft_alibi import TFTConfig
        new_config = TFTConfig(
            vocab_size=common_params['vocab_size'],
            n_layers=common_params['n_layer'],  # New uses n_layers
            n_heads=common_params['n_head'],    # New uses n_heads
            d_model=common_params['n_embd'],    # New uses d_model
            block_size=common_params['block_size'],
            dropout=common_params['dropout'],
            bias=common_params['bias'],
            use_value_factorization=common_params['use_v'],
            use_output_projection=common_params['use_proj'],
            max_position_embeddings=64,
        )
        
        print("  ‚úÖ Configs created successfully")
        return True, (original_config, new_config)
        
    except Exception as e:
        print(f"  ‚ùå Config creation failed: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def compare_model_creation():
    """Compare model creation between original and new."""
    print("\nüèóÔ∏è Comparing model creation...")
    
    success, configs = create_equivalent_configs()
    if not success:
        return False, None, None
    
    original_config, new_config = configs
    
    try:
        # Create original model
        from src.models.model_token_factored_alibi import FactoredTransformerModelALiBi
        torch.manual_seed(42)
        original_model = FactoredTransformerModelALiBi(original_config)
        original_params = original_model.get_num_params()
        print(f"  ‚úÖ Original model: {original_params:,} parameters")
        
        # Create new model
        from models.tft_alibi import TokenFactoredTransformer
        torch.manual_seed(42)
        new_model = TokenFactoredTransformer(new_config)
        new_params = new_model.get_num_params()
        print(f"  ‚úÖ New model: {new_params:,} parameters")
        
        # Compare parameter counts
        param_diff = abs(original_params - new_params)
        param_diff_pct = param_diff / original_params * 100
        
        if param_diff_pct < 5:  # Allow 5% difference
            print(f"  ‚úÖ Parameter counts similar (diff: {param_diff_pct:.1f}%)")
        else:
            print(f"  ‚ö†Ô∏è Parameter counts differ significantly (diff: {param_diff_pct:.1f}%)")
        
        return True, original_model, new_model
        
    except Exception as e:
        print(f"  ‚ùå Model creation failed: {e}")
        import traceback
        traceback.print_exc()
        return False, None, None

def compare_forward_pass(original_model, new_model):
    """Compare forward pass outputs."""
    print("\n‚û°Ô∏è Comparing forward pass...")
    
    try:
        # Create test input
        batch_size, seq_len = 2, 16
        vocab_size = 1000
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
        
        # Set models to eval mode
        original_model.eval()
        new_model.eval()
        
        with torch.no_grad():
            # Original model forward pass
            try:
                original_output = original_model(input_ids=input_ids, labels=input_ids)
                print(f"  ‚úÖ Original forward pass: {original_output['logits'].shape}")
                print(f"      Loss: {original_output['loss'].item():.4f}")
            except Exception as e:
                print(f"  ‚ùå Original forward pass failed: {e}")
                return False
            
            # New model forward pass
            try:
                new_output = new_model(input_ids, labels=input_ids)
                print(f"  ‚úÖ New forward pass: {new_output['logits'].shape}")
                print(f"      Loss: {new_output['loss'].item():.4f}")
            except Exception as e:
                print(f"  ‚ùå New forward pass failed: {e}")
                return False
        
        # Compare shapes
        if original_output['logits'].shape == new_output['logits'].shape:
            print("  ‚úÖ Output shapes match")
        else:
            print(f"  ‚ùå Output shapes differ: {original_output['logits'].shape} vs {new_output['logits'].shape}")
            return False
        
        # Compare loss magnitudes (should be similar)
        loss_diff = abs(original_output['loss'].item() - new_output['loss'].item())
        if loss_diff < 1.0:  # Allow reasonable difference for random initialization
            print(f"  ‚úÖ Loss values reasonable (diff: {loss_diff:.4f})")
        else:
            print(f"  ‚ö†Ô∏è Loss values differ significantly (diff: {loss_diff:.4f})")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Forward pass comparison failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_generation(original_model, new_model):
    """Compare text generation capabilities."""
    print("\nüéØ Comparing generation...")
    
    try:
        batch_size, seq_len = 1, 8
        vocab_size = 1000
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
        
        original_model.eval()
        new_model.eval()
        
        with torch.no_grad():
            # Original model generation
            try:
                original_generated = original_model.generate(
                    input_ids, 
                    max_new_tokens=5,
                    temperature=0.8,
                    top_k=40
                )
                print(f"  ‚úÖ Original generation: {original_generated.shape}")
            except Exception as e:
                print(f"  ‚ùå Original generation failed: {e}")
                return False
            
            # New model generation
            try:
                new_generated = new_model.generate(
                    input_ids,
                    max_new_tokens=5, 
                    temperature=0.8,
                    top_k=40
                )
                print(f"  ‚úÖ New generation: {new_generated.shape}")
            except Exception as e:
                print(f"  ‚ùå New generation failed: {e}")
                return False
        
        # Compare shapes
        if original_generated.shape == new_generated.shape:
            print("  ‚úÖ Generated sequence shapes match")
        else:
            print(f"  ‚ùå Generated shapes differ: {original_generated.shape} vs {new_generated.shape}")
            return False
        
        # Check that original input is preserved
        if torch.equal(original_generated[:, :seq_len], input_ids):
            print("  ‚úÖ Original sequence preserved in generation")
        else:
            print("  ‚ö†Ô∏è Original sequence not preserved")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Generation comparison failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def compare_special_features(original_model, new_model):
    """Compare TFT-specific features."""
    print("\nüîß Comparing TFT-specific features...")
    
    try:
        # Check ALiBi slopes
        original_slopes = original_model.transformer.h[0].attn.alibi_slopes
        new_slopes = new_model.blocks[0].attention.alibi_slopes
        
        if torch.allclose(original_slopes, new_slopes, atol=1e-6):
            print("  ‚úÖ ALiBi slopes match exactly")
        elif original_slopes.shape == new_slopes.shape:
            print("  ‚úÖ ALiBi slopes have same shape (values may differ due to implementation)")
        else:
            print(f"  ‚ùå ALiBi slopes differ: {original_slopes.shape} vs {new_slopes.shape}")
            return False
        
        # Check factorization parameters
        original_has_v = hasattr(original_model.transformer.h[0].attn, 'v_tmp')
        new_has_v = hasattr(new_model.blocks[0].attention, 'v_factorization')
        
        if original_has_v == new_has_v:
            print(f"  ‚úÖ Value factorization consistency: {original_has_v}")
        else:
            print(f"  ‚ö†Ô∏è Value factorization differs: original={original_has_v}, new={new_has_v}")
        
        # Check projection parameters  
        original_has_proj = hasattr(original_model.transformer.h[0].attn, 'proj_tmp')
        new_has_proj = hasattr(new_model.blocks[0].attention, 'output_factorization')
        
        if original_has_proj == new_has_proj:
            print(f"  ‚úÖ Output projection consistency: {original_has_proj}")
        else:
            print(f"  ‚ö†Ô∏è Output projection differs: original={original_has_proj}, new={new_has_proj}")
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå Feature comparison failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run complete comparison."""
    print("=" * 60)
    print("üî¨ TFT MODEL COMPARISON TEST")
    print("=" * 60)
    
    success_count = 0
    total_tests = 5
    
    # Test imports
    success, _ = test_imports()
    if success:
        success_count += 1
    
    # Test model creation
    success, original_model, new_model = compare_model_creation()
    if success:
        success_count += 1
        
        # Only run remaining tests if models were created successfully
        if original_model and new_model:
            # Test forward pass
            if compare_forward_pass(original_model, new_model):
                success_count += 1
            
            # Test generation
            if compare_generation(original_model, new_model):
                success_count += 1
            
            # Test special features
            if compare_special_features(original_model, new_model):
                success_count += 1
    
    # Summary
    print("\n" + "=" * 60)
    print("üìã COMPARISON SUMMARY")
    print("=" * 60)
    
    print(f"Tests passed: {success_count}/{total_tests}")
    
    if success_count == total_tests:
        print("üéâ Perfect match! Refactored model maintains all functionality")
        print("‚úÖ Safe to proceed with the refactored version")
    elif success_count >= total_tests - 1:
        print("‚úÖ Very good match! Minor differences are acceptable")
        print("‚úÖ Refactored model is functionally equivalent")
    else:
        print("‚ö†Ô∏è Significant differences found")
        print("‚ùå Review and fix issues before proceeding")
    
    return success_count >= total_tests - 1

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)